{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CefejINJ8u3"
      },
      "source": [
        "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Code_challenge.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6XGlMyYJ8u7"
      },
      "source": [
        "## Integrated Project: Understanding the yield\n",
        "© ExploreAI Academy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0R3LM1MJ8u8"
      },
      "source": [
        "In this coding challenge, we will apply all of the skills we learned in regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59mSNZoGJ8u8"
      },
      "source": [
        "⚠️ **Note that this code challenge is graded and will contribute to your overall marks for this module. Submit this notebook for grading. Note that the names of the functions are different in this notebook. Transfer the code in your notebook to this submission notebook**\n",
        "\n",
        "### Instructions\n",
        "\n",
        "- **Do not add or remove cells in this notebook. Do not edit or remove the `### START FUNCTION` or `### END FUNCTION` comments. Do not add any code outside of the functions you are required to edit. Doing any of this will lead to a mark of 0%!**\n",
        "\n",
        "- Answer the questions according to the specifications provided.\n",
        "\n",
        "- Use the given cell in each question to see if your function matches the expected outputs.\n",
        "\n",
        "- Do not hard-code answers to the questions.\n",
        "\n",
        "- The use of StackOverflow, Google, and other online tools is permitted. However, copying a fellow student's code is not permissible and is considered a breach of the Honour code. Doing this will result in a mark of 0%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mmuly3uxJ8u9"
      },
      "source": [
        "# Introduction to simple linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INj7xOPIJ8u9"
      },
      "source": [
        "Simple linear regression is a fundamental statistical method used to quantify the relationship between two variables. It allows us to predict an outcome (dependent variable) based on the value of one predictor (independent variable). In this challenge, we will apply simple linear regression to understand how different environmental factors affect the standardised yield of crops.\n",
        "\n",
        "Our insights will not only help local farmers maximise their harvests but also contribute to the sustainable agriculture practices in Maji Ndogo.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9gU5YevJ8u-"
      },
      "source": [
        "# Initial data exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYyjjqsTJ8u-"
      },
      "source": [
        "Before we sow the seeds of our regression model, we need to get to know our soil – the dataset. This dataset was developed through extensive agricultural surveys conducted at farms across Maji Ndogo. It contains various factors that might influence a farm's crop yield, from the elevation of the fields to the average temperature they bask in.\n",
        "\n",
        "Spend some time looking at the data dictionary and start thinking about what could be influencing our crop yield."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HGc6BI9J8u-"
      },
      "source": [
        "# Data dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLjcD4JvJ8u-"
      },
      "source": [
        "**1. Geographic features**\n",
        "\n",
        "- **Field_ID:** A unique identifier for each field (BigInt).\n",
        "\n",
        "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
        "\n",
        "- **Latitude:** Geographical latitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
        "\n",
        "- **Longitude:** Geographical longitude of the field in degrees (Float). (DUMMY VARIABLE- the simulation might have created a relationship)\n",
        "\n",
        "- **Location:** Province the field is in (Text).\n",
        "\n",
        "- **Slope:** The slope of the land in the field (Float).\n",
        "\n",
        "**2. Weather features**\n",
        "\n",
        "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
        "\n",
        "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
        "\n",
        "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).(DUMMY VARIABLE)\n",
        "\n",
        "- **Ave_temps:** Average temperature in Celcius (Float).\n",
        "\n",
        "**3. Soil and crop features**\n",
        "\n",
        "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
        "\n",
        "- **Soil_type:** Type of soil present in the field (Text).\n",
        "\n",
        "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
        "\n",
        "**4. Farm management features**\n",
        "\n",
        "- **Field_ID:** Corresponding field identifier (BigInt).\n",
        "\n",
        "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
        "\n",
        "- **Plot_size:** Size of the plot in the field (Ha) (Float). (DUMMY VARIABLE)\n",
        "\n",
        "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
        "\n",
        "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield (DUMMY VARIABLE - Removed)\n",
        "\n",
        "<br>\n",
        "\n",
        "**5. Target variable**\n",
        "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSDEWNr-J8u_"
      },
      "source": [
        "Let's import our database again, like we did last time. We won't use the weather data so it is commented out.\n",
        "\n",
        "**Important:** Ensure that `data_ingestion.py` file and the `field_data_processor.py` files are stored in the same folder as your notebook, otherwise the data import will fail. The links to the files are below:\n",
        "\n",
        "[Download files here](https://github.com/Explore-AI/Public-Data/raw/master/Maji_Ndogo/modules.zip)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MT5AKENJ8u_"
      },
      "outputs": [],
      "source": [
        "# Read the database, and clean the data using the processing modules we built.\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from field_data_processor import FieldDataProcessor\n",
        "# from weather_data_processor import WeatherDataProcessor\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "\n",
        "config_params = {\n",
        "    \"sql_query\": \"\"\"\n",
        "            SELECT *\n",
        "            FROM geographic_features\n",
        "            LEFT JOIN weather_features USING (Field_ID)\n",
        "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
        "            LEFT JOIN farm_management_features USING (Field_ID)\n",
        "            \"\"\",\n",
        "    \"db_path\": 'sqlite:///Maji_Ndogo_farm_survey_small.db',\n",
        "    \"columns_to_rename\": {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'},\n",
        "    \"values_to_rename\": {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'},\n",
        "    \"weather_csv_path\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\",\n",
        "    \"weather_mapping_csv\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\",\n",
        "    \"regex_patterns\" : {\n",
        "            'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
        "            'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
        "            'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
        "            },\n",
        "}\n",
        "# Ignoring the field data for now.\n",
        "field_processor = FieldDataProcessor(config_params)\n",
        "field_processor.process()\n",
        "field_df = field_processor.df\n",
        "\n",
        "# We're not going to use the weather data this time, so we'll ignore it.\n",
        "# weather_processor = WeatherDataProcessor(config_params)\n",
        "# weather_processor.process()\n",
        "# weather_df = weather_processor.weather_df\n",
        "\n",
        "dataset = field_df.drop(\"Weather_station\", axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctfhdYwAJ8vB"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tR9V15hJ8vC"
      },
      "source": [
        "Before diving into our analysis, it's crucial to ensure the integrity of our dataset and that the data is still as we expect it to be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZX5egijJ8vC"
      },
      "outputs": [],
      "source": [
        "# Validate the data\n",
        "# !pip install pytest\n",
        "\n",
        "dataset.to_csv('sampled_field_df.csv', index=False)\n",
        "\n",
        "!pytest validate_data.py -v\n",
        "\n",
        "import os# Define the file paths\n",
        "field_csv_path = 'sampled_field_df.csv'\n",
        "\n",
        "# Delete sampled_field_df.csv if it exists\n",
        "if os.path.exists(field_csv_path):\n",
        "    os.remove(field_csv_path)\n",
        "    print(f\"Deleted {field_csv_path}\")\n",
        "else:\n",
        "    print(f\"{field_csv_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBNqWhyRJ8vC"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JycUl5saJ8vD"
      },
      "source": [
        "## Challenge 1: Visualising the relationship"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEWw8TpAJ8vD"
      },
      "source": [
        "With our data ready and loaded, it's time to start exploring.\n",
        "\n",
        "Our goal is to determine whether any of the features in our dataset are influencing the `Standard_yield` of a farm. If we can figure out what these relationships are, then we can use them to start predicting what future yields will be, based on these features.\n",
        "\n",
        "For this analysis, we want to find whether any features have a linear relationship with `Standard_yield` so that we can fit a linear regression model to the data. This is important because if we try and fit a linear regression model to non-linear data, our predictions won't be good.\n",
        "\n",
        "Any of the features could have an impact on the `Standard_yield`. Let's begin with `Ave_temps`, the average temperature of the region, and its relationship to `Standard_yield`.\n",
        "\n",
        "Let's start with the basics: a scatter plot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JStFkisyJ8vD"
      },
      "source": [
        "**⚙️ Your task:**\n",
        "\n",
        " 1. Generate a scatter plot to visualise the relationship between `Ave_temps` and `Standard_yield`.\n",
        " 2. Reflect on the scatter plot. Does it suggest a linear relationship, or is the story more complex?\n",
        "\n",
        "**Note:**\n",
        "- Use `matplotlib` to create the scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1XpGRBzJ8vE"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBfYdFAbJ8vE"
      },
      "outputs": [],
      "source": [
        "# Insert code to draw a scatter plot here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgSQ51AuJ8vE"
      },
      "source": [
        "Now, let's write a function to calculate the Pearson correlation coefficient.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "Create a function named `get_correlation` that:\n",
        "1. Takes a DataFrame and the names of the columns we want to determine the correlation for as parameters (`Ave_temps` and `Standard_yield`).\n",
        "2. Calculates the Pearson correlation coefficient between these two columns to quantify their linear relationship.\n",
        "4. Returns the Pearson correlation coefficient.\n",
        "\n",
        "**Note:**\n",
        "- Use `scipy` to calculate the Pearson correlation coefficient.\n",
        "- Ensure your function returns the Pearson correlation coefficient as a float."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkhxQchJJ8vF"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtCwNP-tJ8vF"
      },
      "outputs": [],
      "source": [
        "### START FUNCTION\n",
        "def get_correlation(df, col1, col2):\n",
        "\n",
        "    # Add code to calculate and return the correlation coefficient\n",
        "\n",
        "### END FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOIsjbxKJ8vF"
      },
      "source": [
        "Input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IbpYvJjJ8vF"
      },
      "outputs": [],
      "source": [
        "correlation = get_correlation(dataset,'Ave_temps','Standard_yield')\n",
        "print(\"Pearson correlation coefficient:\", correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmiqbpSPJ8vG"
      },
      "source": [
        "Expected output\n",
        "```\n",
        "Correlation: 0.006785950289020164\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uLDpKR-J8vG"
      },
      "source": [
        "What do you notice about the scatter plot and the dispersion of data points? It's essential to visualise our data first; if the data doesn't follow a linear pattern, then a linear regression model may fail to accurately capture the underlying relationship. The correlation also seems extremly low, what does this tell us?\n",
        "\n",
        "Let's write down some of our observations:\n",
        "\n",
        "  - ✍️ Your notes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPbshRxOJ8vG"
      },
      "source": [
        "## Challenge 2: A breath of fresh data: Pollution as a predictor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLk4oetPJ8vH"
      },
      "source": [
        "It's time to shift our gaze from the warmth of the sun to the haze of pollution. Could the levels of pollution, a concern for farmers and environmentalists alike, be an indicator of our yields?\n",
        "\n",
        "Let's begin by fitting a simple linear regression model, to try and capture the linear relationship between these columns.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "Create a function named `fit_linear_regression_model` that:\n",
        "1. Takes in a DataFrame and the names of the `Pollution_level` and `Standard_yield` columns.\n",
        "2. Fits a linear regression model to the data.\n",
        "3. Returns the model, the model predictions and the actual y-values.\n",
        "\n",
        "**Note:**\n",
        "- Use `LinearRegression` from `sklearn` to fit the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4dkjfFLJ8vH"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDaT9JtqJ8vI"
      },
      "outputs": [],
      "source": [
        "### START FUNCTION\n",
        "def fit_linear_regression_model(df, pollution_col, yield_col):\n",
        "\n",
        "    # Add code to fit the linear regression model and return the model, predictions, y-values.\n",
        "\n",
        "### END FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG0F3Zb1J8vI"
      },
      "source": [
        "Input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7SWpCSGJ8vI"
      },
      "outputs": [],
      "source": [
        "model, predictions, y_values = fit_linear_regression_model(dataset, 'Pollution_level', 'Standard_yield')\n",
        "print(f\"Model: {model}\")\n",
        "print(f\"Predictions: {predictions}\")\n",
        "print(f\"Actual Y-Values: {y_values}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaVmcyjIJ8vJ"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "- Model: an instance of the LinearRegression class.\n",
        "- Predictions: a NumPy array of predicted values.\n",
        "- y: a Pandas Series with the actual target values used for training.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nSnUgvMJ8vK"
      },
      "source": [
        "Linear regression models only work well if our data is in fact linear. So, lets create a scatter plot to visualise the relationship between pollution and crop yields. In addition to this, let's use the predictions from the model we fit to add the line of best fit to our scatter plot.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "1. Generate a scatter plot to visualise the effect that pollution has on standard yield.\n",
        "2. Draw the line of best fit\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "You can use this line of code to draw the regression line on the plot:\n",
        "`plt.plot(X, predictions, color='red', label='Regression line')`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u5K8Vz9eJ8vK"
      },
      "outputs": [],
      "source": [
        "X = dataset[['Pollution_level']]\n",
        "y = dataset['Standard_yield']\n",
        "\n",
        "# Add code to draw the scatter plot and the regression line"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecnSG0M9J8vK"
      },
      "source": [
        "Now, use the get_correlation() function that we defined earlier to test the correlation between `Pollution_level` and `Standard_yield`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ol1ctthxJ8vL"
      },
      "outputs": [],
      "source": [
        "Pollution_correlation = get_correlation(dataset,'Pollution_level','Standard_yield')\n",
        "print(\"Pearson correlation coefficient:\", Pollution_correlation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkkbUc7eJ8vL"
      },
      "source": [
        "Expected output\n",
        "```\n",
        "Correlation: -0.2857609646210543\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppcY7pZMJ8vL"
      },
      "source": [
        "Reflect on the difference between this plot and correlation and the previous one with the average temperature. Is the relationship between pollution and yield more linear?\n",
        "\n",
        "  - ✍️ Your notes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1D3qoXQJ8vM"
      },
      "source": [
        "\n",
        "We can also gain a better understanding of our model by examining the slope and intercept.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "Create a function named `get_slope_intercept` that:\n",
        "1. Inputs the `model` we fitted and calculates the slope and intercept of the line of best fit.\n",
        "2. Return the slope and intercept as a tuple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwYLj0QxJ8vM"
      },
      "outputs": [],
      "source": [
        "### START FUNCTION\n",
        "def get_slope_intercept(model):\n",
        "\n",
        "    #Add code to calcualte and return the slope and intercept\n",
        "\n",
        "### END FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5Az-S4sJ8vN"
      },
      "source": [
        "Input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YQDM6CQJ8vN"
      },
      "outputs": [],
      "source": [
        "slope, intercept = get_slope_intercept(model)\n",
        "print(\"Slope:\", slope)\n",
        "print(\"Intercept:\", intercept)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KOOxzSAGJ8vN"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "Slope: -0.1427617720986604\n",
        "Intercept: 0.5662684415393379\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elk05VnlJ8vO"
      },
      "source": [
        "\n",
        "What does the slope tell us about the strength of the relationship between pollution and yield? Also, what can we learn from the y-intercept?\n",
        "\n",
        "  - ✍️ Your notes here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN71vppOJ8vO"
      },
      "source": [
        "## Challenge 3: The haze clears: Evaluating pollution's predictive power"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcNZSn9lJ8vP"
      },
      "source": [
        "When we look at the scatterplots of `Standard_yield` with `Ave_temps` and `Pollution_level`, it appears that pollution level might have a more linear relationship.  This means that we could potentially use a simple linear regression model to make predictions about the yield of a farm based on its pollution level. However, before we do this we need to further assess the strength of the linear relationship between `Pollution_level` and `Standard_yield`.\n",
        "\n",
        "Let's assess our model's performance using R-squared, Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "Create a function named `calculate_evaluation_metrics` that:\n",
        "1. Takes the predictions and y-values from our fitted model as input.\n",
        "2. Calculates and returns the R-squared, Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE) of the model's performance.\n",
        "\n",
        "**Note:**\n",
        "1. Calculate the model's performance metrics using the entire dataset.\n",
        "2. Return the evaluation metrics as a tuple in the order: R-squared, MAE, MSE, RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hqfe_BPOJ8vP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4KUfOC7J8vQ"
      },
      "outputs": [],
      "source": [
        "### START FUNCTION\n",
        "def calculate_evaluation_metrics(predictions, y_values):\n",
        "\n",
        "   # Add code to calculate and return the r2, mae, mse and rmse\n",
        "\n",
        "### END FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXWGn8lcJ8vQ"
      },
      "source": [
        "Input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj5KgfQsJ8vR"
      },
      "outputs": [],
      "source": [
        "evaluation_metrics = calculate_evaluation_metrics(predictions, y_values)\n",
        "print(f\"Evaluation Metrics:\\nR-squared: {evaluation_metrics[0]}\\nMAE: {evaluation_metrics[1]}\\nMSE: {evaluation_metrics[2]}\\nRMSE: {evaluation_metrics[3]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6unQngiJ8vR"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "A tuple containing numerical values for R-squared, MAE, MSE, and RMSE (give or take 0.0001):\n",
        "\n",
        "```python\n",
        "R-squared: 0.08165932890115546\n",
        "MAE: 0.08554642090904992\n",
        "MSE: 0.011477732254034848\n",
        "RMSE: 0.10713417873878928\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYDTVefdJ8vS"
      },
      "source": [
        "Think about what these metrics tell us about our model's accuracy and reliability. Write down your observations:\n",
        "\n",
        "  - ✍️ Your notes here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGogE7eMJ8vS"
      },
      "source": [
        "## Challenge 4: The dividing line: Train-test split in action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YzhDALhJ8vS"
      },
      "source": [
        "As we delve deeper into the relationship between `Pollution_level` and `Standard_yield`, we must ensure our model is not merely memorising the data but truly understanding it. This brings us to the pivotal technique of Train-Test Split.\n",
        "\n",
        "**The importance of train-test split**\n",
        "\n",
        "Imagine teaching a student for an exam by using the very questions that will appear on it. They might score perfectly, but does it mean they've truly learned? Similarly, a model might perform exceptionally on the data it was trained on, but the real test of knowledge comes from unseen data. This is where the train-test split comes in, allowing us to assess our model's generalisation capabilities by training on one subset of data and testing on another.\n",
        "\n",
        "**Your task**\n",
        "\n",
        "Create a function named `data_train_test_split` that:\n",
        "1. Takes in the DataFrame and the two columns we want to model the relationship between (`Pollution_level` and `Standard_yield`).\n",
        "2. Separates it into features (`X`) based on `Pollution_level` and the target (`y`) based on `Standard_yield`.\n",
        "3. Splits the data into training and testing sets using an 80-20 split and sets `random_state = 42` for reproducibility.\n",
        "4.  Returns a tuple containing: `X_train` and `X_test`, which are DataFrames containing features for training and testing, respectively, along with `y_train` and `y_test`, which are Series representing subsets of the original DataFrame's target variable for training and testing.\n",
        "\n",
        "**Note:**\n",
        "- Use `train_test_split` from `sklearn.model_selection` to split the data.\n",
        "- Train a linear regression model on the training set using `LinearRegression` from `sklearn.linear_model`.\n",
        "- If the random state is not set to `42` the code will not be marked correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VAg-uTjJ8vT"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMztmv7dJ8vT"
      },
      "outputs": [],
      "source": [
        "### START FUNCTION\n",
        "def data_train_test_split(df, pollution_col, yield_col):\n",
        "\n",
        "   # Add code to calculate and return the X_train, X_test, y_train and y_test\n",
        "\n",
        "### END FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7c5BDjfJ8vU"
      },
      "source": [
        "Input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oz9oVqzqJ8vU"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = data_train_test_split(dataset, 'Pollution_level', 'Standard_yield')\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lD5Cit0J8vV"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "- X_train: DataFrame, subset of the original DataFrame's features for training.\n",
        "- X_test: DataFrame, subset of the original DataFrame's features for testing.\n",
        "- y_train: Series, subset of the original DataFrame's target variable for training.\n",
        "- y_test: Series, subset of the original DataFrame's target variable for testing.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0970XexJ8vV"
      },
      "source": [
        "Now lets fit a linear regression model to the data.\n",
        "\n",
        "**Your task**\n",
        "\n",
        "Create a function named `train_split_linear_regression_model()` that:\n",
        "1. Takes `X_train`, `X_test`, `y_train`, `y_test` as input (the results from the `data_train_test_split()` function).\n",
        "2. Trains a simple linear regression model on the training set.\n",
        "3. Uses the testing set to make predictions.\n",
        "4. Returns a tuple containing: the model, the predictions and y_test (the actual y values in the testing set values) ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHdUmOlUJ8vW"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VeIhvYnJ8vW"
      },
      "outputs": [],
      "source": [
        "### START FUNCTION\n",
        "def train_split_linear_regression_model(X_train, X_test, y_train, y_test):\n",
        "\n",
        "    # Add code to fit the linear regression model and return the model, predictions and y_test\n",
        "\n",
        "### END FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJjHExlxJ8vX"
      },
      "source": [
        "Input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghVRpGieJ8vX"
      },
      "outputs": [],
      "source": [
        "train_test_model, predictions_test, y_test = train_split_linear_regression_model(X_train, X_test, y_train, y_test)\n",
        "print(f\"Train-Test Model: {train_test_model}\")\n",
        "print(f\"Test Predictions: {predictions_test}\")\n",
        "print(f\"Test Actual Y-Values: {y_test}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DldgJ4kWJ8vX"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "- Model: an instance of the LinearRegression class.\n",
        "- Predictions: a NumPy array of predicted values.\n",
        "- y_test: a Pandas Series with the actual target values used for evaluating the model.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7bi9bzwJ8vX"
      },
      "source": [
        "Now, let's evaluate our use our train-test model by determining R-squared, MAE, MSE, and RMSE.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "1. Use the `calculate_evaluation_metrics` function (defined in Challenge 3) to calculate the R-squared, MAE, MSE, and RMSE.\n",
        "2. The function should return a tuple containing the evaluation metrics (R-squared, MAE, MSE, and RMSE).\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Ensure to use the test set to calculate the evaluation metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6Ii79YLJ8vY"
      },
      "outputs": [],
      "source": [
        "# Add code to calculate the R-squared, MAE, MSE, and RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ktukwaPJ8vY"
      },
      "source": [
        "Expected output:\n",
        "\n",
        "A tuple containing numerical values for R-squared, MAE, MSE, and RMSE (give or take 0.0001):\n",
        "\n",
        "```python\n",
        "R-squared: 0.08065722992150859\n",
        "MAE:  0.08794942119747501\n",
        "MSE: 0.012250634233355654\n",
        "RMSE: 0.11068258324305434\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM2mu1EsJ8vY"
      },
      "source": [
        "Reflect on the difference between these metics and the metrics we obtained from the previous model (that was not split into training and testings sets). Why do you think the fit is worse now? And, why should we choose the worse option? (Reflect on the course material if these answers to these questions are not clear.)\n",
        "\n",
        "  - ✍️ Your notes here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQB2WQA8J8vZ"
      },
      "source": [
        "## Challenge 5: Diagnosing model fit through residual analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBn0fhkIJ8vZ"
      },
      "source": [
        "From our analysis, it seems as though neither `Ave_temps` or `Pollution_level` have a strong linear fit with `Standard_yield`. However, even if we had obtained good results from our evaluation metrics, there are still other crucial assumptions we need to verify to ensure our model is well-fitted. Residual analysis plays a pivotal role in diagnosing the fit of linear regression models, helping us understand whether the assumptions of linearity, independence, and homoscedasticity (constant variance) of residuals are met.\n",
        "\n",
        "If they are not met, can we confidently model this problem using the model? And why?\n",
        "\n",
        "  - ✍️ Your notes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihMNXHocJ8va"
      },
      "source": [
        "First, let's create a histogram.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "1. Calucate the residuals of our train test model (difference between `y_test` and `predictions_test`)\n",
        "2. Plot these residuals as a histogram to assess their distribution and identify any patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPoQbgEoJ8vb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFMSUy__J8vb"
      },
      "outputs": [],
      "source": [
        "residuals = y_test - predictions_test # calculating the residuals\n",
        "\n",
        "#  Add code to create a histogram of residuals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnE9AxxoJ8vb"
      },
      "source": [
        "What does the histogram tell us about our data:\n",
        "\n",
        "1. Examine the shape of the histogram. What does the distribution of residuals tell us about the normality of the data? Consider whether the residuals appear to be symmetrically distributed around zero.\n",
        "\n",
        "    - ✍️ Your notes here\n",
        "\n",
        "2. Compare the tails of the histogram to a normal distribution. Are there signs of heavy tails or skewness that could affect the reliability of the regression model's predictions?\n",
        "\n",
        "    - ✍️ Your notes here\n",
        "\n",
        "3. Assess the centering of the histogram around the zero line. How does this central tendency reflect on the bias of the model's predictions?\n",
        "\n",
        "    - ✍️ Your notes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNn9atSKJ8vc"
      },
      "source": [
        "Now, let's create a scatter plot of these residuals against the predicted values.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "1. Create a scatter plot of the residuals against the predicted values - `predictions_test` should be on the x-axis and the `residuals` on the y-axis.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- A horizontal line can be added at y=0 to make it easier to see if the residuals are evenly distributed around zero by adding this line of code:\n",
        "`plt.axhline(y=0, color='r', linestyle='--')`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEOwrBJzJ8vc"
      },
      "outputs": [],
      "source": [
        "#  Add code to create a scatter plot of residuals against the predicted values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T89A8i8ZJ8vc"
      },
      "source": [
        "Relfect on what the scatter plot tells us about our data and the fit of the model:\n",
        "\n",
        "1. Analyse the scatter plot for any apparent patterns or structures in the data. What does this suggest about the appropriateness of the linear regression model for the dataset?\n",
        "\n",
        "    - ✍️ Your notes here\n",
        "\n",
        "2. Inspect the plot for signs of heteroscedasticity. How does the spread of residuals change as the predicted values increase? What might this imply about the constant variance assumption in linear regression?\n",
        "\n",
        "    - ✍️ Your notes here\n",
        "\n",
        "3. Identify whether the residuals are evenly scattered above and below the zero line across the range of predicted values. What can this tell us about the model's performance in terms of bias and prediction accuracy?\n",
        "\n",
        "    - ✍️ Your notes here\n",
        "\n",
        "4. Look for outliers or clusters of points that deviate significantly from the majority. How might these points influence the overall fit of the model?\n",
        "\n",
        "    - ✍️ Your notes here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CCeXdk5J8vd"
      },
      "source": [
        "Our final task is to examine the mean and standard deviation of the residuals, which provide further insights into the model's performance.\n",
        "\n",
        "**⚙️ Your task:**\n",
        "\n",
        "Create a function named `calculate_residuals_statistics` that:\n",
        "1. Uses the `predictions_test` and `y_test` (obtained from Challenge 4) to calculate the residuals.\n",
        "2. Calculates the mean and standard deviation of the residuals.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "- Use `numpy` for the mean and standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlWgjXM8J8ve"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeJlBjXHJ8ve"
      },
      "outputs": [],
      "source": [
        "### START FUNCTION\n",
        "def calculate_residuals_statistics(predictions, y_test):\n",
        "\n",
        "    # Add code to calculate and return the mean_residual and std_residual\n",
        "\n",
        "### END FUNCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L38Q71aAJ8vf"
      },
      "source": [
        "Input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rs2Pjuo6J8vf"
      },
      "outputs": [],
      "source": [
        "mean_residual, std_residual = calculate_residuals_statistics(predictions_test, y_test)\n",
        "print(f\"Mean: {mean_residual}\\nStandard deviation: {std_residual}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212t1nOSJ8vf"
      },
      "source": [
        "Expected output:\n",
        "```\n",
        "Mean: 0.0058580231923217015\n",
        "Standard deviation: 0.11052745268770957\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqAoOgkWJ8vg"
      },
      "source": [
        "What does the mean of the residuals tell us about the bias in our predictions? How does a mean close to zero reflect on our model's accuracy?\n",
        "\n",
        "- ✍️ Your notes here\n",
        "\n",
        "What does the standard deviation of the residuals indicate about the variability of our predictions? Why is it important for this value to be relatively low?\n",
        "\n",
        " - ✍️ Your notes here\n",
        "\n",
        "What are the potential consequences of a high standard deviation of residuals on the reliability of the model's predictions? How might this affect our confidence in the model's estimates?\n",
        "\n",
        "- ✍️ Your notes here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_T_sg_9JJ8vg"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Linear regression, for all its strengths, assumes a straightforward relationship between the predictor and the outcome. Yet, the natural world seldom adheres to such simplicity. Factors influencing crop yields in Maji Ndogo—be it temperature, rainfall, or pollution—interact in complex, often nonlinear ways. Our initial model with `Ave_temps` hinted at this complexity, suggesting that the effect of the average temperature on yields might follow a more intricate pattern than a straight line can depict (or no pattern at all).\n",
        "\n",
        "Our yield also depends on more than just the pollution or the temperature, it depends on many of the factors. From our EDA we could see that. We also saw that not all crops are affected equally by pollution or temperature, so we could simplify our model if we remove the influence of the different crops. Once your submission is done, as a challenge to yourself, try to split the data again by crop type (with a loop) and use the functions you created to loop over all of the crop types and print out your metrics.\n",
        "\n",
        "Compare them, and discuss your results with your colleagues. Is there a crop type that is affected by pollution more than other crop types?\n",
        "\n",
        "As we dive deeper into regression, it's crucial to remember that with each model comes a new perspective. Just as a farmer selects the tool that best suits the task at hand, so must we choose our models with intention and insight. Exploring beyond linear regression opens up new vistas of understanding, allowing us to capture the richness of relationships within our data.\n",
        "\n",
        "In the fields of Maji Ndogo and beyond, countless stories await. It's up to us, with curiosity as our guide and an ever-expanding array of models at our disposal, to uncover them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "754xACgDJ8vh"
      },
      "source": [
        "#  \n",
        "\n",
        "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
        "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}