{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xpIPMNPoU_-p"
   },
   "source": [
    "# Examples: Introduction NLP and cleaning text\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore text-cleaning and feature extraction techniques in NLP. We'll use the NLTK library to preprocess unstructured text data, preparing it for machine learning tasks. By using examples, we'll work through the steps of cleaning text data and extracting meaningful features.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "* Gain a basic understanding of text-cleaning techniques.\n",
    "* Implement text-cleaning steps such as removing URLs, converting text to lowercase, and removing punctuation.\n",
    "* Understand the concept of tokenisation and its importance in text processing.\n",
    "* Apply stemming and lemmatisation to reduce words to their root forms.\n",
    "* Demonstrate the removal of stop words and their impact on text analysis.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This tutorial introduces basic concepts in Natural Language Processing and in particular, common techniques for handling, processing and preparing unstructured text data for use with machine learning models. The concepts introduced here are also useful for text analysis, so please feel free to do more research and see what can be achieved using the MBTI dataset.\n",
    "\n",
    "Before diving in, let's acquire the necessary data and the primary library we'll be utilising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lKmhMwlDU_-u"
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubXxX-njU_-u"
   },
   "source": [
    "NLTK - Natural Language Toolkit - is a widely used library for building Python programs to work with human language data. It provides interfaces to numerous corpora and lexical resources, such as WordNet, along with a suite of text-processing libraries for classification, tokenisation, stemming, tagging, parsing, and semantic reasoning. Additionally, NLTK offers wrappers for various NLP libraries and features an active discussion forum.\n",
    "\n",
    "Thanks to a hands-on guide introducing programming fundamentals alongside topics in computational linguistics, plus comprehensive API documentation, NLTK is suitable for linguists, engineers, students, educators, researchers, and industry users alike. NLTK is available for Windows, Mac OS X, and Linux. Best of all, NLTK is a free, open-source, community-driven project.\n",
    "\n",
    "Let's import `nltk` and other packages to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  - \u001b[36mnltk\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "!poetry add nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OQm0O5XHU_-z"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# set plot style\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading NLTK Corpora"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C68jzOFpU_-2"
   },
   "source": [
    "\n",
    "Several text processing methods introduced in the NLTK require accessing predefined language resources, such as [stopword lists](https://www.geeksforgeeks.org/removing-stop-words-nltk-python/). For instance, when identifying stopwords within a text, NLTK relies on a [corpus](https://en.wikipedia.org/wiki/Text_corpus) containing such words. This corpus serves as a reference for the lookup operation during text processing. To ensure seamless execution of NLTK methods, it's essential to download the required corpora beforehand. Failure to do so may result in lookup errors during tokenisation and stopword removal processes. Fortunately, we can preemptively address these issues by downloading the necessary corpora using the NLTK downloader tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 680
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29356,
     "status": "error",
     "timestamp": 1560340175121,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "w8Iw1yCRU_-2",
    "outputId": "188501d1-fcf6-45be-8a45-56f885491dcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ultracode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ultracode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK corpora\n",
    "#nltk.download\n",
    "# or we can download directly, i.e.\n",
    "nltk.download(['punkt','stopwords'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lct3bK9aU_-7"
   },
   "source": [
    "You should see a pop-up box similar to the one shown below.\n",
    "\n",
    "Note: The box might appear in the background, in which case you can use alt + tab to switch to the downloader window.\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Pictures/blob/master/nltk_downloader.png?raw=true\" width=50%/> \n",
    "\n",
    "Use it to navigate to the items we need to download:\n",
    "\n",
    "* stopwords corpus (under the Corpora tab)\n",
    "* punkt tokenizer models (under the Models tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the download was successful, then the following import should work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we'll explore stopwords in greater detail later in this train, it won't hurt to take a quick look at what we've downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "# Get the list of English stopwords\n",
    "stopwords_list = stopwords.words('english')\n",
    "print(stopwords_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The MBTI dataset\n",
    "\n",
    "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides people into one of 16 distinct personality types across 4 axes:\n",
    "\n",
    "    Introversion (I) – Extroversion (E)\n",
    "    Intuition (N) – Sensing (S)\n",
    "    Thinking (T) – Feeling (F)\n",
    "    Judging (J) – Perceiving (P)\n",
    "\n",
    "[(More can be learned about what these mean here)](https://www.myersbriggs.org/my-mbti-personality-type/mbti-basics/home.htm)\n",
    "\n",
    "So for example, someone who prefers introversion, intuition, thinking and perceiving would be labelled an INTP in the MBTI system, and there are lots of personality-based components that would model or describe this person’s preferences or behaviour based on their label.\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/1/1f/MyersBriggsTypes.png'>\n",
    "\n",
    "Image by Jake Beech, [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0)\n",
    "\n",
    "In this train, we'll use a version of [the MBTI dataset](https://www.kaggle.com/datasnaek/mbti-type) which contains over 6000 rows of data, where on each row is a person’s:\n",
    "\n",
    " - MBTI type (4 letter MBTI code)\n",
    " - A section of each of the last 50 things they have posted online (Each entry separated by \"|||\" (3 pipe characters))   \n",
    " \n",
    "_**Note:** If you are curious, you can find out what your MBTI personality is by taking the test here: https://www.16personalities.com/_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKeKPQWkU_-9"
   },
   "source": [
    "### Let's get the data and clean it up a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 763,
     "status": "error",
     "timestamp": 1560340182462,
     "user": {
      "displayName": "Bryan Davies",
      "photoUrl": "",
      "userId": "03059035420523728518"
     },
     "user_tz": -120
    },
    "id": "1g8hjHxFU_-9",
    "outputId": "07b8ba22-a4e3-4d73-c121-55cd4a827b10"
   },
   "outputs": [],
   "source": [
    "# Read the MBTI dataset\n",
    "mbti = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/mbti_train.csv')\n",
    "mbti.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by printing a list of all the MBTI personality types that are present in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print list of unique MBTI personality types\n",
    "type_labels = list(mbti.type.unique())\n",
    "print(type_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDSv8UyDU__G"
   },
   "source": [
    "Let's have a look at how many data samples we have for each of the different MBTI personality types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3G_aB4hU__G",
    "outputId": "92110f2c-5cab-4f7b-a5b3-4a9b0be103e3"
   },
   "outputs": [],
   "source": [
    "# Visualise the distribution of MBTI personality types\n",
    "mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7h-hcmsU__J"
   },
   "source": [
    "It looks like we have very few samples for the 'ES' types. Maybe because they are out in the real-world, not sitting behind a computer screen! :)   \n",
    "   \n",
    "Let's increase the size of the dataset by separating each of the 50 posts in the `posts` column of each row into its own row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "88DRAzHnU__J",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate each post in the 'posts' column into its own row\n",
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hF5aiupbU__M",
    "outputId": "897367e2-ffbb-4ced-f418-54ed1178e646"
   },
   "outputs": [],
   "source": [
    "# how many rows do we have now?\n",
    "all_mbti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bElSlHKGU__W",
    "outputId": "182b0f14-6348-49f7-ff80-63515865a30a"
   },
   "outputs": [],
   "source": [
    "all_mbti['type'].value_counts().plot(kind = 'bar')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnKe3gyoU__Z"
   },
   "source": [
    "Although the proportions of the classes remain consistent, we have significantly increased the number of samples for the 'ES' personality types by separating each post into its own row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VCXae5QXU__Z"
   },
   "source": [
    "## text-cleaning\n",
    "\n",
    "### Removing Noise"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwJMs441U__a"
   },
   "source": [
    "In text-analytics, removing noise (i.e. unnecessary information) is a key part of getting the data into a usable format.  Some techniques are standard, but your own unique dataset will require some creative thinking on your part.\n",
    "\n",
    "For the MBTI dataset, we will be doing the following:\n",
    "* Removing the web-urls\n",
    "* Making all the text lower case\n",
    "* Removing punctuation\n",
    "\n",
    "**[Regular expressions](https://www.regular-expressions.info/)** can be very useful for extracting information from text.  If you feel brave, go teach yourself all about it. If not, just follow along.  This next step effectively removes all websites and replaces them with the text `'web-url'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "lwQ40KKDU__a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Replace URLs in the 'post' column with a placeholder string\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PLOKzwsdU__d",
    "outputId": "04f9009e-4ec0-4e6d-91a5-9f5fb59b497f"
   },
   "outputs": [],
   "source": [
    "all_mbti.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-anTzPrnU__h"
   },
   "source": [
    "**Food for thought...** There seem to be a lot of YouTube and other links embedded.  Maybe you can think of ways to collect even more information from these links?  How about page titles and names of YouTube videos?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wzM8TbWBU__h"
   },
   "source": [
    "### Remove punctuation\n",
    "\n",
    "First we make all the text lowercase to remove some noise from capitalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CUATZmo5U__h",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_mbti['post'] = all_mbti['post'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove the punctuation using the `string` import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvhXxcHzU__j",
    "outputId": "7d82c9f5-e448-4f1c-edf9-c75493ea7cf6"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "A-RPGgE5U__l",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(post):\n",
    "    return ''.join([l for l in post if l not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2sFFdyNCU__n"
   },
   "outputs": [],
   "source": [
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)\n",
    "all_mbti['post'].iloc[268558]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like some punctuation sneaked in! See if you can figure out why? Hint it has something to do with the standard encoding on text files in python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FvA-QZmRU__r"
   },
   "source": [
    "### Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRDkqnGgU__s"
   },
   "source": [
    "A tokeniser divides text into a sequence of tokens, which roughly correspond to \"words\" (see the [Stanford Tokeniser](https://nlp.stanford.edu/software/tokenizer.html)). We will use tokenisers to clean up the data, making it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "X9nnIjyhU__t",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "stR6cyC_U__v",
    "outputId": "2c2ed231-656c-420f-dd04-fe19d8df1d09"
   },
   "outputs": [],
   "source": [
    "word_tokenize('A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\".')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the TreeBankWordTokenizer since it is MUCH quicker than the word_tokenize function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "V2rChKdjU__y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize the text using the TreebankWordTokenizer\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "all_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8o8u2kwU__1",
    "outputId": "cf9aad76-1846-4af1-aef1-399bd4a31edc"
   },
   "outputs": [],
   "source": [
    "all_mbti['tokens'].iloc[55555]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hAUkklVXU__6"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qlw3iqz3U__7"
   },
   "source": [
    "Stemming is the process of transforming to the root word. It uses an algorithm that removes\n",
    "common word-endings from English words, such as “ly,” “es,” “ed,” and “s.” \n",
    "\n",
    "For instance, suppose you're conducting an analysis and you wish to treat words like \"carefully,\" \"cared,\" \"cares,\" and \"caringly\" as a single entity, \"care,\" rather than separate words. There are three widely used stemming algorithms, namely:\n",
    "* Porter\n",
    "* Lancaster\n",
    "* Snowball\n",
    "\n",
    "Out of these three, we will be using the `SnowballStemmer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BRY-EGeuU__7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "q5hmBs_DU__-",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "words = 'caring cares cared caringly carefully'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ILU4MAgaVAAA",
    "outputId": "e012205b-1a2a-441a-f614-1dd1ee498ce6"
   },
   "outputs": [],
   "source": [
    "# find the stem of each word in words\n",
    "stemmer = SnowballStemmer('english')\n",
    "for word in words.split():\n",
    "    print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us stem all of the words in the MBTI dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0iw9106FVAAE",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mbti_stemmer(words, stemmer):\n",
    "    return [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjTuPQNOVAAF"
   },
   "outputs": [],
   "source": [
    "all_mbti['stem'] = all_mbti['tokens'].apply(mbti_stemmer, args=(stemmer, ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the results of the stemmer to see what we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['stem'][i]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rFln-NFtVAAI"
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is another text normalisation technique that aims to reduce words to their base or dictionary form, known as lemmas. Unlike stemming, which simply removes word endings, lemmatization considers the context of the word and morphological analysis to accurately derive the lemma. This ensures that the resulting lemma is a valid word found in the dictionary, preserving the semantic meaning of the word in different contexts. While lemmatization is more computationally intensive compared to stemming, it offers higher accuracy in maintaining the integrity of words.\n",
    "\n",
    "Sometimes, we may end up with a word that closely resembles the original, while in other cases, we might get a word that is entirely different. Let's explore some examples to illustrate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nivlmH3rVAAJ",
    "outputId": "44c3133a-81b6-4c75-cc46-f4e8ef7f387c"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize(\"cats\"))\n",
    "print(lemmatizer.lemmatize(\"cacti\"))\n",
    "print(lemmatizer.lemmatize(\"geese\"))\n",
    "print(lemmatizer.lemmatize(\"rocks\"))\n",
    "print(lemmatizer.lemmatize(\"python\"))\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"run\"))\n",
    "print(lemmatizer.lemmatize(\"ran\",'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's lemmatize all of the words in the MBTI dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "A8ejyfxbVAAK",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def mbti_lemma(words, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(word) for word in words]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "sxUiwXfIVAAN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "all_mbti['lemma'] = all_mbti['tokens'].apply(mbti_lemma, args=(lemmatizer, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will print out the results of the lemmatization to see what we have done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S3loMYtFVAAO",
    "outputId": "12581d00-60bf-409e-b554-02530fd535b3"
   },
   "outputs": [],
   "source": [
    "for i, t in enumerate(all_mbti.iloc[268702]['tokens']):    \n",
    "    print ('{:20s} --> {:10s}'.format(t, all_mbti.iloc[268702]['lemma'][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZomXVzoVAAR"
   },
   "source": [
    "### Stop Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_7g3o3SVAAR"
   },
   "source": [
    "Stop-words are words which do not contain important significance to be used in Search Queries. Usually these words are filtered out from search queries because they return a vast amount of unnecessary information. `nltk` has a corpus of stopwords. Let's print out the stopwords for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XY1p3uZVAAS"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rD6ovPGLVAAW",
    "outputId": "aae2892f-0e10-4325-f3a7-3e7091352282"
   },
   "outputs": [],
   "source": [
    "sorted(stopwords.words('english'))[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function removes all of the English stopwords from the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "GPDdMJ2SVAAX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_stop_words(tokens):    \n",
    "    return [t for t in tokens if t not in stopwords.words('english')]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G5PBwZRUVAAb"
   },
   "source": [
    "To remove stop words, simply uncomment and execute the following cell! Please note, however, that this process may take a while due to the computational load imposed by the pandas apply function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kyeHeXlAVAAb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# all_mbti['stem'] = all_mbti['tokens'].apply(remove_stop_words)\n",
    "#all_mbti['stem']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "\n",
    "In conclusion, mastering text-cleaning techniques and feature extraction in Natural Language Processing (NLP) is crucial for effectively preparing unstructured text data for analysis and machine learning tasks. Through methods such as removing noise like URLs, converting text to lowercase, removing punctuation, tokenisation, stemming, lemmatization, and stop word removal, we can enhance the quality of text data and extract meaningful features for further analysis. These techniques play a fundamental role in NLP pipelines, enabling the development of robust models and gaining valuable insights from textual data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VCXae5QXU__Z",
    "wzM8TbWBU__h",
    "FvA-QZmRU__r",
    "hAUkklVXU__6",
    "rFln-NFtVAAI",
    "UZomXVzoVAAR",
    "qp-n688CVAAc",
    "tGmGzrbsVAAf",
    "oFzCFS89VABM",
    "TlO1q-zlVABg"
   ],
   "name": "3_How-do-machines-understand language.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
