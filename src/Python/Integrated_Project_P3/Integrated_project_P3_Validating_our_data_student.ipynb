{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4b7a61",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Code_challenge.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Integrated project: Validating our data\n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this Code Challenge we’re diving into the agricultural dataset again to continue to validate our data. Before we do that, we’re pausing to build a data pipeline that will ingest and clean our data with the press of a button, cleaning up our code significantly. Once that’s ready, we’ll complete our data validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d133774e",
   "metadata": {},
   "source": [
    "⚠️ **NOTE that this code challenge is graded and will contribute to your overall marks for this module. Submit this notebook for grading. Note that the names of the functions are different in this notebook. Transfer the code in your notebook to this submission notebook**\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- **Do not add or remove cells in this notebook. Do not edit or remove the `### START FUNCTION` or `### END FUNCTION` comments. Do not add any code outside of the functions you are required to edit. Doing any of this will lead to a mark of 0%!**\n",
    "\n",
    "- Answer the questions according to the specifications provided.\n",
    "\n",
    "- Use the given cell in each question to see if your function matches the expected outputs.\n",
    "\n",
    "- Do not hard-code answers to the questions.\n",
    "\n",
    "- The use of StackOverflow, Google, and other online tools is permitted. However, copying a fellow student's code is not permissible and is considered a breach of the Honour code. Doing this will result in a mark of 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944ccbc",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2e633f",
   "metadata": {},
   "source": [
    "Let's pick up where we left off, shall we?\n",
    "\n",
    "Recall our previous disappointment - the mismatch between our dataset and the weather station data? That was a curveball, wasn't it? Half of our measurements were out of range, raising eyebrows and doubts alike. Our quest for data validation had hit a snag, but as any seasoned data scientist will tell you, every problem is a hidden opportunity waiting to be discovered.\n",
    "\n",
    "I'm sure you have some ideas on how to solve this, so I'll share mine...\n",
    "\n",
    "One thing that bugs me is the tolerance level I chose. Why 1.5%? \n",
    "\n",
    "We saw that half of our means were not within that tolerance, but why would it be within 1.5% and not 5%?\n",
    "\n",
    "Although we may think 1.5% is a good measurement of accuracy, there may be errors in each dataset, and more variation in one than the other, which could make the means differ by more than 1.5% even if the objective truth is that the means are the same. We need to be a little more scientific with our approach and account for the difference in the data. I am sure you know what I am about to say next, right?\n",
    "\n",
    "**Hypothesis testing** takes into account both the means and the variances of the distributions being compared. The variance here is crucial because it gives us insight into the spread of the data points around the means for our two datasets. Two samples could have the same mean but very different variances, leading to different interpretations of their similarities or differences.\n",
    "\n",
    "Our main goal is the same: Is the data in our `MD_agric_df` dataset representative of reality? To answer this, we use weather-related data from nearby stations to validate our results. If the weather data matches the data we have, we can be more confident that our dataset represents reality. \n",
    "\n",
    "So what's the plan? \n",
    "1. Create a null hypothesis.\n",
    "1. Import the `MD_agric_df` dataset and clean it up.\n",
    "1. Import the weather data.\n",
    "1. Map the weather data to the field data.\n",
    "1. Calculate the means of the weather station dataset and the means of the main dataset.\n",
    "2. Calculate all the parameters we need to do a t-test. \n",
    "3. Interpret our results.\n",
    "\n",
    "Do you see a bit of repetition here? Steps 2-5 are a repeat of last time. The quick and dirty fix is to copy that code across to our notebook, but what about next time and the time after that? All of that code will also clutter this notebook and make our analysis harder to read.\n",
    "\n",
    "You might also think of exporting the fixed and merged data from last time to a CSV, but what if there is new data in the database?\n",
    "\n",
    "So, let's stop for a second and think about how we can make this simpler, more extendible and reusable in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147be850",
   "metadata": {},
   "source": [
    "# Data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe8e55e",
   "metadata": {},
   "source": [
    "**1. Geographic features**\n",
    "\n",
    "- **Field_ID:** A unique identifier for each field (BigInt).\n",
    " \n",
    "- **Elevation:** The elevation of the field above sea level in metres (Float).\n",
    "\n",
    "- **Latitude:** Geographical latitude of the field in degrees (Float).\n",
    "\n",
    "- **Longitude:** Geographical longitude of the field in degrees (Float).\n",
    "\n",
    "- **Location:** Province the field is in (Text).\n",
    "\n",
    "- **Slope:** The slope of the land in the field (Float).\n",
    "\n",
    "**2. Weather features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Rainfall:** Amount of rainfall in the area in mm (Float).\n",
    "\n",
    "- **Min_temperature_C:** Average minimum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Max_temperature_C:** Average maximum temperature recorded in Celsius (Float).\n",
    "\n",
    "- **Ave_temps:** Average temperature in Celcius (Float).\n",
    "\n",
    "**3. Soil and crop features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Soil_fertility:** A measure of soil fertility where 0 is infertile soil, and 1 is very fertile soil (Float).\n",
    "\n",
    "- **Soil_type:** Type of soil present in the field (Text).\n",
    "\n",
    "- **pH:** pH level of the soil, which is a measure of how acidic/basic the soil is (Float).\n",
    "\n",
    "**4. Farm management features**\n",
    "\n",
    "- **Field_ID:** Corresponding field identifier (BigInt).\n",
    "\n",
    "- **Pollution_level:** Level of pollution in the area where 0 is unpolluted and 1 is very polluted (Float).\n",
    "\n",
    "- **Plot_size:** Size of the plot in the field (Ha) (Float).\n",
    "\n",
    "- **Chosen_crop:** Type of crop chosen for cultivation (Text).\n",
    "\n",
    "- **Annual_yield:** Annual yield from the field (Float). This is the total output of the field. The field size and type of crop will affect the Annual Yield\n",
    "\n",
    "- **Standard_yield:** Standardised yield expected from the field, normalised per crop (Float). This is independent of field size, or crop type. Multiplying this number by the field size, and average crop yield will give the Annual_Yield.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Weather_station_data (CSV)**\n",
    "\n",
    "- **Weather_station_ID:** The weather station the data originated from. (Int)\n",
    "\n",
    "- **Message:** The weather data was captured by sensors at the stations, in the format of text messages.(Str)\n",
    "\n",
    "**Weather_data_field_mapping (CSV)**\n",
    "\n",
    "- **Field_ID:** The id of the field that is connected to a weather station. This is the key we can use to join the weather station ID to the original data. (Int)\n",
    "\n",
    "- **Weather_station_ID:** The weather station that is connected to a field. If a field has `weather_station_ID = 0` then that field is closest to weather station 0. (Int)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cca3f5",
   "metadata": {},
   "source": [
    "# Dealing with a friendly warning from Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70849ec4",
   "metadata": {},
   "source": [
    "If you are running this notebook in `Python 3.12` or later, you might get a warning if you run the imports below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c121c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5564ea8",
   "metadata": {},
   "source": [
    "If you are lucky enough to see this warning, it let's us know that Pandas is changing soon and will require another package to be installed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c05b1b",
   "metadata": {},
   "source": [
    "```python\n",
    "...2334042735.py:3: DeprecationWarning: \n",
    "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
    "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
    "but was not found to be installed on your system.\n",
    "If this would cause problems for you,\n",
    "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
    "        \n",
    "  import pandas as pd\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501ddc37",
   "metadata": {},
   "source": [
    "We can safely ignore these warnings, but soon our script will fail to import Pandas, so let's fix it today, and we won't have to worry about it for a long time. The warning tells us that Pyarrow will soon be a requirement to import Pandas, so we can just install it with pip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab50c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7c0e7",
   "metadata": {},
   "source": [
    "# Cleaning up our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a858a84",
   "metadata": {},
   "source": [
    "When we pulled in the data last time, we actually made an assumption that our script worked. We assumed that all the fixes we made to the data actually made it in, and assumed that the database didn't change. But what if someone added more records, fixed the data on the database that we were making in our notebook, or added a new column of data? \n",
    "\n",
    "Last time we started to build what's known as a **data pipeline**. We often do this as data scientists. The data we work with is almost always stored in databases, and we don't want to transform, clean up or make changes to the database unless it is really beneficial. So we retrieve the data from the database, and in our workspace, we shape the data into a useful format for our goal. \n",
    "\n",
    "Last time we imported data with this code: (⚠️ Don't run it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff3fd6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msqlalchemy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_engine, text \u001b[38;5;66;03m# Importing the SQL interface. If this fails, run !pip install sqlalchemy in another cell.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Create an engine for the database\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#engine = create_engine('sqlite:///Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches.\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd # importing the Pandas package with an alias, pd\n",
    "from sqlalchemy import create_engine, text # Importing the SQL interface. If this fails, run !pip install sqlalchemy in another cell.\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Create an engine for the database\n",
    "#engine = create_engine('sqlite:///Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b8530",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection object\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Use Pandas to execute the query and store the result in a DataFrame\n",
    "    MD_agric_df = pd.read_sql_query(text(sql_query), connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e4173c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df.rename(columns={'Annual_yield': 'Crop_type_Temp', 'Crop_type': 'Annual_yield'}, inplace=True)\n",
    "MD_agric_df.rename(columns={'Crop_type_Temp': 'Crop_type'}, inplace=True)\n",
    "MD_agric_df['Elevation'] = MD_agric_df['Elevation'].abs()\n",
    "\n",
    "# Correcting 'Crop_type' column\n",
    "def correct_crop_type(crop):\n",
    "    crop = crop.strip()  # Remove trailing spaces\n",
    "    corrections = {\n",
    "        'cassaval': 'cassava',\n",
    "        'wheatn': 'wheat',\n",
    "        'teaa': 'tea'\n",
    "    }\n",
    "    return corrections.get(crop, crop)  # Get the corrected crop type, or return the original if not in corrections\n",
    "\n",
    "# Apply the correction function to the Crop_type column\n",
    "MD_agric_df['Crop_type'] = MD_agric_df['Crop_type'].apply(correct_crop_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df10cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97717c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importing the regex pattern\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "patterns = {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }\n",
    "\n",
    "def extract_measurement(message):\n",
    "    \"\"\"\n",
    "    Extracts a numeric measurement value from a given message string.\n",
    "\n",
    "    The function applies regular expressions to identify and extract\n",
    "    numeric values related to different types of measurements such as\n",
    "    Rainfall, Average Temperatures, and Pollution Levels from a text message.\n",
    "    It returns the key of the matching record, and first matching value as a floating-point number.\n",
    "    \n",
    "    Parameters:\n",
    "    message (str): A string message containing the measurement information.\n",
    "\n",
    "    Returns:\n",
    "    float: The extracted numeric value of the measurement if a match is found;\n",
    "           otherwise, None.\n",
    "\n",
    "    The function uses the following patterns for extraction:\n",
    "    - Rainfall: Matches numbers (including decimal) followed by 'mm', optionally spaced.\n",
    "    - Ave_temps: Matches numbers (including decimal) followed by 'C', optionally spaced.\n",
    "    - Pollution_level: Matches numbers (including decimal) following 'Pollution at' or '='.\n",
    "    \n",
    "    Example usage:\n",
    "    extract_measurement(\"【2022-01-04 21:47:48】温度感应: 现在温度是 12.82C.\")\n",
    "    # Returns: 'Temperature', 12.82\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, pattern in patterns.items(): # Loop through all of the patterns and check if it matches the pattern value.\n",
    "        match = re.search(pattern, message)\n",
    "        if match:\n",
    "            # Extract the first group that matches, which should be the measurement value if all previous matches are empty.\n",
    "            # print(match.groups()) # Uncomment this line to help you debug your regex patterns.\n",
    "            return key, float(next((x for x in match.groups() if x is not None)))\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d829c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n",
    "\n",
    "weather_station_means = weather_station_df.groupby(by = ['Weather_station_ID','Measurement'])['Value'].mean(numeric_only = True)\n",
    "weather_station_means = weather_station_means.unstack()\n",
    "weather_station_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e22228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this line of code to see which messages are not assigned yet.\n",
    "weather_station_df[(weather_station_df['Measurement'] == None)|(weather_station_df['Value'].isna())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afa1e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MD_agric_df = MD_agric_df.merge(weather_station_mapping_df,on = 'Field_ID', how='left')\n",
    "MD_agric_df.drop(columns=\"Unnamed: 0\")\n",
    "MD_agric_df_weather_means = MD_agric_df.groupby(\"Weather_station\").mean(numeric_only = True)[['Pollution_level','Rainfall', 'Ave_temps']]\n",
    "\n",
    "MD_agric_df_weather_means = MD_agric_df_weather_means.rename(columns = {'Ave_temps':\"Temperature\"})\n",
    "MD_agric_df_weather_means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b59f3d6",
   "metadata": {},
   "source": [
    "We have to copy that code across from another notebook, and we may miss some blocks or lines of code. We may not copy over the code in the right order, and we may not have documented the code well either. \n",
    "\n",
    "Do you also think this is a massive block of code that feels like it is in the way of our analysis? Is it simple to follow and simple to change? \n",
    "\n",
    "No! So let's spend a bit of time building a proper data pipeline that imports the data from the different sources we have, cleans up the data, and tests whether our data is what we expect it to be. We want to do all of this in a way that doesn't cause our notebook to be filled with thousands of lines of code and become a nightmare to debug. As a final step, we want to automate a few simple data validation checks in our code.\n",
    "\n",
    "To do this we're going to re-organise or **refactor** our code into modules. We're going to reorganise all of the code into smaller modules so our code is more readable, maintainable and extendable. \n",
    "\n",
    "We can create a module to interact with the database, a module to transform and clean the field-related data and another module to process the weather data.\n",
    "\n",
    "<br>\n",
    "\n",
    "So here's the plan: \n",
    "\n",
    "1. Gather all of the code from our last \"pipeline\".\n",
    "\n",
    "2. Re-organise the code into our new three modules: \n",
    "\n",
    "    a. `data_ingesation.py` - All SQL-related functions, and web-based data retrieval.\n",
    "\n",
    "    b. `field_data_processor.py` - All transformations, cleanup, and merging functionality.\n",
    "\n",
    "    c. `weather_data_processor.py` - All transformations and cleanup of the weather station data.\n",
    "\n",
    "3. Copy our code into the modules and test their functionality.\n",
    "\n",
    "4. Create automated data validation tests to ensure our data is as we expect it to be.\n",
    "\n",
    "Once we're done with that, we're going to jump into the reason why we're here. So let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a0d5b",
   "metadata": {},
   "source": [
    "# Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a491c",
   "metadata": {},
   "source": [
    "Why are we doing this? Well, we want to reduce our data pipeline code to a couple of lines like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c68f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor.process()\n",
    "field_df = field_processor.weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b766dd0",
   "metadata": {},
   "source": [
    "We're going to need a bit more code, but we want to press a button and go from databases and CSVs to a Pandas DataFrame. All of the code and processes are happening in the modules and we get the end result. The second big motivator is to make our code more modular. If we want to debug a problem in the field data, we know where to go, and if we want to import other IoT weather sensors we can just modify the weather data module.\n",
    "\n",
    "The first challenge; automating the data ingestion. There are two places we're fetching data:\n",
    "1. SQLite database - We need to create an SQLite engine, connect to the database, run a query and return a pandas DataFrame.\n",
    "2. Web CSV file - Read the CSV data from the web, and import it as a DataFrame.\n",
    "\n",
    "So let's start building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e22624",
   "metadata": {},
   "source": [
    "## Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0bffbb",
   "metadata": {},
   "source": [
    "Ok, so first up, let's grab the code that interacted with the database, and the web CSV files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9a92a",
   "metadata": {},
   "source": [
    "SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0f0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # importing the Pandas package with an alias, pd\n",
    "from sqlalchemy import create_engine, text # Importing the SQL interface. If this fails, run !pip install sqlalchemy in another cell.\n",
    "\n",
    "\n",
    "# Create an engine for the database\n",
    "engine = create_engine('sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches.\n",
    "\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection object\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Use Pandas to execute the query and store the result in a DataFrame\n",
    "    MD_agric_df = pd.read_sql_query(text(sql_query), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6700c2c",
   "metadata": {},
   "source": [
    "CSV files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21df87d7",
   "metadata": {},
   "source": [
    "Creating modules in Jupyter notebooks is a bit of a pain. If we make changes to the `module.py` file, we have to restart the notebook kernel and import the module again in order to apply those changes. So, we're going to fully develop it, test it in this notebook, and only then, move it to a `data_ingestion.py` file, and import it. \n",
    "\n",
    "To create a module, our code should ideally be encapsulated in functions or classes. How do we choose?\n",
    "\n",
    "Since the process of connecting to a database, and querying some data is relatively straightforward, functions seemed like the best fit. Functions allow us to encapsulate the necessary steps in clear, reusable blocks of code without the overhead of managing class objects. It's like using a **simple tool** for a specific task — pick it up, use it, and put it back without needing to remember anything about the last use.\n",
    "\n",
    "On the other hand, our **data processing** modules are a bit more complex. These modules not only perform various operations on the data but also need to keep track of the data as it goes through these processes. For this, we use **classes** to **create DataFrame objects** as attributes. This approach simplifies data handling since we're passing the object around, which inherently knows its data and the operations it can perform within the class.\n",
    "\n",
    "This idea ties back to OOP. Class objects are designed to deal with data and operations on that data via methods, while functions are made to do the simpler tasks.\n",
    "\n",
    "So for **data ingestion**, we're just going to use functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7947076",
   "metadata": {},
   "source": [
    "So our **main task** will be to **convert the data ingestion code into functions** that we can call from the module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e6de7",
   "metadata": {},
   "source": [
    "So this code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an engine for the database\n",
    "engine = create_engine('sqlite:///Maji_Ndogo_farm_survey_small.db') #Make sure to have the .db file in the same directory as this notebook, and the file name matches.\n",
    "\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "# Create a connection object\n",
    "with engine.connect() as connection:\n",
    "    \n",
    "    # Use Pandas to execute the query and store the result in a DataFrame\n",
    "    MD_agric_df = pd.read_sql_query(text(sql_query), connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9248b4e",
   "metadata": {},
   "source": [
    "Becomes neat, modular functions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea12a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_engine(db_path):\n",
    "    engine = create_engine(db_path)\n",
    "    return engine\n",
    "\n",
    "def query_data(engine, sql_query):\n",
    "    with engine.connect() as connection:\n",
    "        df = pd.read_sql_query(text(sql_query), connection)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8258e138",
   "metadata": {},
   "source": [
    "So if we call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a0d4832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_db_engine('sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bff0102",
   "metadata": {},
   "source": [
    "We get the SQL engine object which we can use with the query to connect to the database, and run a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b0125d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40734</td>\n",
       "      <td>786.05580</td>\n",
       "      <td>-7.389911</td>\n",
       "      <td>-7.556202</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>14.795113</td>\n",
       "      <td>1125.2</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>6.169393</td>\n",
       "      <td>8.526684e-02</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.751354</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.577964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30629</td>\n",
       "      <td>674.33410</td>\n",
       "      <td>-7.736849</td>\n",
       "      <td>-1.051539</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.374611</td>\n",
       "      <td>1450.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.676648</td>\n",
       "      <td>3.996838e-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.069865</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.486302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39924</td>\n",
       "      <td>826.53390</td>\n",
       "      <td>-9.926616</td>\n",
       "      <td>0.115156</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.339692</td>\n",
       "      <td>2208.9</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>28.4</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.331993</td>\n",
       "      <td>3.580286e-01</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.208801</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.649647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5754</td>\n",
       "      <td>574.94617</td>\n",
       "      <td>-2.420131</td>\n",
       "      <td>-6.592215</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>7.109855</td>\n",
       "      <td>328.8</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>5.328150</td>\n",
       "      <td>2.866871e-01</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.277635</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.532348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14146</td>\n",
       "      <td>886.35300</td>\n",
       "      <td>-3.055434</td>\n",
       "      <td>-7.952609</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>55.007656</td>\n",
       "      <td>785.2</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.721234</td>\n",
       "      <td>4.319027e-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.555076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>11472</td>\n",
       "      <td>681.36145</td>\n",
       "      <td>-7.358371</td>\n",
       "      <td>-6.254369</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>16.213196</td>\n",
       "      <td>885.7</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.741063</td>\n",
       "      <td>3.286828e-01</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.609930</td>\n",
       "      <td>potato</td>\n",
       "      <td>0.554482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>19660</td>\n",
       "      <td>667.02120</td>\n",
       "      <td>-3.154559</td>\n",
       "      <td>-4.475046</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>2.397553</td>\n",
       "      <td>501.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.445833</td>\n",
       "      <td>1.602583e-01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.812289</td>\n",
       "      <td>maize</td>\n",
       "      <td>0.438194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>41296</td>\n",
       "      <td>670.77900</td>\n",
       "      <td>-14.472861</td>\n",
       "      <td>-6.110221</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>7.636470</td>\n",
       "      <td>1586.6</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.385873</td>\n",
       "      <td>8.221326e-09</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.681629</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.800776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>33090</td>\n",
       "      <td>429.48840</td>\n",
       "      <td>-14.653089</td>\n",
       "      <td>-6.984116</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>13.944720</td>\n",
       "      <td>1272.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Silt</td>\n",
       "      <td>5.562508</td>\n",
       "      <td>6.917245e-10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.659874</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.507595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>8375</td>\n",
       "      <td>763.09030</td>\n",
       "      <td>-4.317028</td>\n",
       "      <td>-6.344461</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>35.189430</td>\n",
       "      <td>516.4</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.087792</td>\n",
       "      <td>2.612715e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.453064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Field_ID  Elevation   Latitude  Longitude        Location      Slope  \\\n",
       "0        40734  786.05580  -7.389911  -7.556202    Rural_Akatsi  14.795113   \n",
       "1        30629  674.33410  -7.736849  -1.051539    Rural_Sokoto  11.374611   \n",
       "2        39924  826.53390  -9.926616   0.115156    Rural_Sokoto  11.339692   \n",
       "3         5754  574.94617  -2.420131  -6.592215  Rural_Kilimani   7.109855   \n",
       "4        14146  886.35300  -3.055434  -7.952609  Rural_Kilimani  55.007656   \n",
       "...        ...        ...        ...        ...             ...        ...   \n",
       "5649     11472  681.36145  -7.358371  -6.254369    Rural_Akatsi  16.213196   \n",
       "5650     19660  667.02120  -3.154559  -4.475046  Rural_Kilimani   2.397553   \n",
       "5651     41296  670.77900 -14.472861  -6.110221   Rural_Hawassa   7.636470   \n",
       "5652     33090  429.48840 -14.653089  -6.984116   Rural_Hawassa  13.944720   \n",
       "5653      8375  763.09030  -4.317028  -6.344461  Rural_Kilimani  35.189430   \n",
       "\n",
       "      Rainfall  Min_temperature_C  Max_temperature_C  Ave_temps  \\\n",
       "0       1125.2               -3.1               33.1      15.00   \n",
       "1       1450.7               -3.9               30.6      13.35   \n",
       "2       2208.9               -1.8               28.4      13.30   \n",
       "3        328.8               -5.8               32.2      13.20   \n",
       "4        785.2               -2.5               31.0      14.25   \n",
       "...        ...                ...                ...        ...   \n",
       "5649     885.7               -4.3               33.4      14.55   \n",
       "5650     501.1               -4.8               32.1      13.65   \n",
       "5651    1586.6               -3.8               33.4      14.80   \n",
       "5652    1272.2               -6.2               34.6      14.20   \n",
       "5653     516.4               -3.8               29.6      12.90   \n",
       "\n",
       "      Soil_fertility Soil_type        pH  Pollution_level  Plot_size  \\\n",
       "0               0.62     Sandy  6.169393     8.526684e-02        1.3   \n",
       "1               0.64  Volcanic  5.676648     3.996838e-01        2.2   \n",
       "2               0.69  Volcanic  5.331993     3.580286e-01        3.4   \n",
       "3               0.54     Loamy  5.328150     2.866871e-01        2.4   \n",
       "4               0.72     Sandy  5.721234     4.319027e-02        1.5   \n",
       "...              ...       ...       ...              ...        ...   \n",
       "5649            0.61     Sandy  5.741063     3.286828e-01        1.1   \n",
       "5650            0.54     Sandy  5.445833     1.602583e-01        8.7   \n",
       "5651            0.64  Volcanic  5.385873     8.221326e-09        2.1   \n",
       "5652            0.63      Silt  5.562508     6.917245e-10        1.3   \n",
       "5653            0.64     Sandy  5.087792     2.612715e-01        0.5   \n",
       "\n",
       "      Crop_type Annual_yield  Standard_yield  \n",
       "0      0.751354      cassava        0.577964  \n",
       "1      1.069865      cassava        0.486302  \n",
       "2      2.208801          tea        0.649647  \n",
       "3      1.277635      cassava        0.532348  \n",
       "4      0.832614        wheat        0.555076  \n",
       "...         ...          ...             ...  \n",
       "5649   0.609930       potato        0.554482  \n",
       "5650   3.812289        maize        0.438194  \n",
       "5651   1.681629          tea        0.800776  \n",
       "5652   0.659874      cassava        0.507595  \n",
       "5653   0.226532        wheat        0.453064  \n",
       "\n",
       "[5654 rows x 18 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef469416",
   "metadata": {},
   "source": [
    "We can even call the `create_db_engine()` function inside the `query_data()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66b65d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40734</td>\n",
       "      <td>786.05580</td>\n",
       "      <td>-7.389911</td>\n",
       "      <td>-7.556202</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>14.795113</td>\n",
       "      <td>1125.2</td>\n",
       "      <td>-3.1</td>\n",
       "      <td>33.1</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>6.169393</td>\n",
       "      <td>8.526684e-02</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.751354</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.577964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30629</td>\n",
       "      <td>674.33410</td>\n",
       "      <td>-7.736849</td>\n",
       "      <td>-1.051539</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.374611</td>\n",
       "      <td>1450.7</td>\n",
       "      <td>-3.9</td>\n",
       "      <td>30.6</td>\n",
       "      <td>13.35</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.676648</td>\n",
       "      <td>3.996838e-01</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.069865</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.486302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39924</td>\n",
       "      <td>826.53390</td>\n",
       "      <td>-9.926616</td>\n",
       "      <td>0.115156</td>\n",
       "      <td>Rural_Sokoto</td>\n",
       "      <td>11.339692</td>\n",
       "      <td>2208.9</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>28.4</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.69</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.331993</td>\n",
       "      <td>3.580286e-01</td>\n",
       "      <td>3.4</td>\n",
       "      <td>2.208801</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.649647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5754</td>\n",
       "      <td>574.94617</td>\n",
       "      <td>-2.420131</td>\n",
       "      <td>-6.592215</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>7.109855</td>\n",
       "      <td>328.8</td>\n",
       "      <td>-5.8</td>\n",
       "      <td>32.2</td>\n",
       "      <td>13.20</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Loamy</td>\n",
       "      <td>5.328150</td>\n",
       "      <td>2.866871e-01</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.277635</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.532348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14146</td>\n",
       "      <td>886.35300</td>\n",
       "      <td>-3.055434</td>\n",
       "      <td>-7.952609</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>55.007656</td>\n",
       "      <td>785.2</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.25</td>\n",
       "      <td>0.72</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.721234</td>\n",
       "      <td>4.319027e-02</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.832614</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.555076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5649</th>\n",
       "      <td>11472</td>\n",
       "      <td>681.36145</td>\n",
       "      <td>-7.358371</td>\n",
       "      <td>-6.254369</td>\n",
       "      <td>Rural_Akatsi</td>\n",
       "      <td>16.213196</td>\n",
       "      <td>885.7</td>\n",
       "      <td>-4.3</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.55</td>\n",
       "      <td>0.61</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.741063</td>\n",
       "      <td>3.286828e-01</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.609930</td>\n",
       "      <td>potato</td>\n",
       "      <td>0.554482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5650</th>\n",
       "      <td>19660</td>\n",
       "      <td>667.02120</td>\n",
       "      <td>-3.154559</td>\n",
       "      <td>-4.475046</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>2.397553</td>\n",
       "      <td>501.1</td>\n",
       "      <td>-4.8</td>\n",
       "      <td>32.1</td>\n",
       "      <td>13.65</td>\n",
       "      <td>0.54</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.445833</td>\n",
       "      <td>1.602583e-01</td>\n",
       "      <td>8.7</td>\n",
       "      <td>3.812289</td>\n",
       "      <td>maize</td>\n",
       "      <td>0.438194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5651</th>\n",
       "      <td>41296</td>\n",
       "      <td>670.77900</td>\n",
       "      <td>-14.472861</td>\n",
       "      <td>-6.110221</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>7.636470</td>\n",
       "      <td>1586.6</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>33.4</td>\n",
       "      <td>14.80</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Volcanic</td>\n",
       "      <td>5.385873</td>\n",
       "      <td>8.221326e-09</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.681629</td>\n",
       "      <td>tea</td>\n",
       "      <td>0.800776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>33090</td>\n",
       "      <td>429.48840</td>\n",
       "      <td>-14.653089</td>\n",
       "      <td>-6.984116</td>\n",
       "      <td>Rural_Hawassa</td>\n",
       "      <td>13.944720</td>\n",
       "      <td>1272.2</td>\n",
       "      <td>-6.2</td>\n",
       "      <td>34.6</td>\n",
       "      <td>14.20</td>\n",
       "      <td>0.63</td>\n",
       "      <td>Silt</td>\n",
       "      <td>5.562508</td>\n",
       "      <td>6.917245e-10</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.659874</td>\n",
       "      <td>cassava</td>\n",
       "      <td>0.507595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5653</th>\n",
       "      <td>8375</td>\n",
       "      <td>763.09030</td>\n",
       "      <td>-4.317028</td>\n",
       "      <td>-6.344461</td>\n",
       "      <td>Rural_Kilimani</td>\n",
       "      <td>35.189430</td>\n",
       "      <td>516.4</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>29.6</td>\n",
       "      <td>12.90</td>\n",
       "      <td>0.64</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>5.087792</td>\n",
       "      <td>2.612715e-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.226532</td>\n",
       "      <td>wheat</td>\n",
       "      <td>0.453064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5654 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Field_ID  Elevation   Latitude  Longitude        Location      Slope  \\\n",
       "0        40734  786.05580  -7.389911  -7.556202    Rural_Akatsi  14.795113   \n",
       "1        30629  674.33410  -7.736849  -1.051539    Rural_Sokoto  11.374611   \n",
       "2        39924  826.53390  -9.926616   0.115156    Rural_Sokoto  11.339692   \n",
       "3         5754  574.94617  -2.420131  -6.592215  Rural_Kilimani   7.109855   \n",
       "4        14146  886.35300  -3.055434  -7.952609  Rural_Kilimani  55.007656   \n",
       "...        ...        ...        ...        ...             ...        ...   \n",
       "5649     11472  681.36145  -7.358371  -6.254369    Rural_Akatsi  16.213196   \n",
       "5650     19660  667.02120  -3.154559  -4.475046  Rural_Kilimani   2.397553   \n",
       "5651     41296  670.77900 -14.472861  -6.110221   Rural_Hawassa   7.636470   \n",
       "5652     33090  429.48840 -14.653089  -6.984116   Rural_Hawassa  13.944720   \n",
       "5653      8375  763.09030  -4.317028  -6.344461  Rural_Kilimani  35.189430   \n",
       "\n",
       "      Rainfall  Min_temperature_C  Max_temperature_C  Ave_temps  \\\n",
       "0       1125.2               -3.1               33.1      15.00   \n",
       "1       1450.7               -3.9               30.6      13.35   \n",
       "2       2208.9               -1.8               28.4      13.30   \n",
       "3        328.8               -5.8               32.2      13.20   \n",
       "4        785.2               -2.5               31.0      14.25   \n",
       "...        ...                ...                ...        ...   \n",
       "5649     885.7               -4.3               33.4      14.55   \n",
       "5650     501.1               -4.8               32.1      13.65   \n",
       "5651    1586.6               -3.8               33.4      14.80   \n",
       "5652    1272.2               -6.2               34.6      14.20   \n",
       "5653     516.4               -3.8               29.6      12.90   \n",
       "\n",
       "      Soil_fertility Soil_type        pH  Pollution_level  Plot_size  \\\n",
       "0               0.62     Sandy  6.169393     8.526684e-02        1.3   \n",
       "1               0.64  Volcanic  5.676648     3.996838e-01        2.2   \n",
       "2               0.69  Volcanic  5.331993     3.580286e-01        3.4   \n",
       "3               0.54     Loamy  5.328150     2.866871e-01        2.4   \n",
       "4               0.72     Sandy  5.721234     4.319027e-02        1.5   \n",
       "...              ...       ...       ...              ...        ...   \n",
       "5649            0.61     Sandy  5.741063     3.286828e-01        1.1   \n",
       "5650            0.54     Sandy  5.445833     1.602583e-01        8.7   \n",
       "5651            0.64  Volcanic  5.385873     8.221326e-09        2.1   \n",
       "5652            0.63      Silt  5.562508     6.917245e-10        1.3   \n",
       "5653            0.64     Sandy  5.087792     2.612715e-01        0.5   \n",
       "\n",
       "      Crop_type Annual_yield  Standard_yield  \n",
       "0      0.751354      cassava        0.577964  \n",
       "1      1.069865      cassava        0.486302  \n",
       "2      2.208801          tea        0.649647  \n",
       "3      1.277635      cassava        0.532348  \n",
       "4      0.832614        wheat        0.555076  \n",
       "...         ...          ...             ...  \n",
       "5649   0.609930       potato        0.554482  \n",
       "5650   3.812289        maize        0.438194  \n",
       "5651   1.681629          tea        0.800776  \n",
       "5652   0.659874      cassava        0.507595  \n",
       "5653   0.226532        wheat        0.453064  \n",
       "\n",
       "[5654 rows x 18 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "df = query_data(create_db_engine('sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db'), sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca659546",
   "metadata": {},
   "source": [
    "This seems simple, but let's think for a second about what could go wrong. For example, what if there is a problem like the database has a schema, and no actual data? Or our query doesn't return any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "892fbeaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Field_ID, Elevation, Latitude, Longitude, Location, Slope, Rainfall, Min_temperature_C, Max_temperature_C, Ave_temps, Soil_fertility, Soil_type, pH, Pollution_level, Plot_size, Crop_type, Annual_yield, Standard_yield]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "WHERE Rainfall < 0 \n",
    "\"\"\"\n",
    "# The last line won't ever be true, so no results will be returned. \n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89985eb4",
   "metadata": {},
   "source": [
    "We get an empty DataFrame, because SQL returned an empty query result. When we try to filter results, we get an answer that would not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea68a976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], Name: Rainfall, dtype: bool)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Rainfall'] > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabaefaf",
   "metadata": {},
   "source": [
    "So to avoid this we need to add error handling into our code so that we stop the process if something is wrong, and tell us what the problem is before we continue. \n",
    "\n",
    "Secondly, to help us understand how our code is executing we're going to add some logs. While print statements can help us to debug our code, we have to remove them once our code goes into use, one by one. `logging` is a better way to debug our code than print statements because we can add `logging.INFO()` logs to know what our code is doing, and `logging.DEBUG()` statements that have more detail in case we want to debug a specific loop in a bit more in detail. There are also various other tools to use, and we can also silence all logging with a single line of code. If we used print statements, we will have to comment them out one by one. \n",
    "\n",
    "If we apply these two ideas, we get the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12778c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b8bb7d",
   "metadata": {},
   "source": [
    "Now when we run the incorrect query again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc433f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 08:58:00,437 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 08:58:00,444 - data_ingestion - ERROR - The query returned an empty DataFrame.\n",
      "2024-02-26 08:58:00,445 - data_ingestion - ERROR - SQL query failed. Error: The query returned an empty DataFrame.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The query returned an empty DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m sql_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124mSELECT *\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124mFROM geographic_features\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mWHERE Rainfall < 0 \u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# The last line won't ever be true, so no results will be returned. \u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mquery_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSQL_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msql_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m df\n",
      "Cell \u001b[0;32mIn[7], line 40\u001b[0m, in \u001b[0;36mquery_data\u001b[0;34m(engine, sql_query)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \n\u001b[1;32m     39\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSQL query failed. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     42\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while querying the database. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 35\u001b[0m, in \u001b[0;36mquery_data\u001b[0;34m(engine, sql_query)\u001b[0m\n\u001b[1;32m     33\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe query returned an empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m     36\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery executed successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[0;31mValueError\u001b[0m: The query returned an empty DataFrame."
     ]
    }
   ],
   "source": [
    "SQL_engine = create_db_engine('sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db')\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "WHERE Rainfall < 0 \n",
    "\"\"\"\n",
    "# The last line won't ever be true, so no results will be returned. \n",
    "\n",
    "df = query_data(SQL_engine, sql_query)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d83421",
   "metadata": {},
   "source": [
    "We get a log of what happened, and we now get an error telling us there is something wrong with the DataFrame, and we are prevented from processing it further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d867e110",
   "metadata": {},
   "source": [
    "Next up, let's include the CSV data handling. This is the original code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b5eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0f7f6",
   "metadata": {},
   "source": [
    "These two files are imported in the same way, so we can use one function to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c8a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "\n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "    \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_data = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b77cfc",
   "metadata": {},
   "source": [
    "Great! Now our code can connect to a database for the field data, use a query to retrieve data and create a DataFrame. We can also import CSV files from a URL into a DataFrame, and avoid pulling unexpected data. If we put all this code together we have the basic structure of our module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0462cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "db_path = 'sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39a85194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 21:36:34,267 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 21:36:34,355 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 21:36:35,298 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-25 21:36:36,038 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    }
   ],
   "source": [
    "# Testing module functions  \n",
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a713ffaa",
   "metadata": {},
   "source": [
    "Once we run this we should get a log telling us that it all worked. \n",
    "\n",
    "Note that there are imports at the top of the cell. When we move this to a module, it needs to import packages like SQL Alchemy and Pandas. \n",
    "\n",
    "Let's do a test to make sure these functions are working well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "977a2cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcc78d4",
   "metadata": {},
   "source": [
    "**Expected outcome:** \n",
    "\n",
    "`field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcd517",
   "metadata": {},
   "source": [
    "Before we go and move this into the `data_ingestion.py` file, we are missing one **crucial** aspect. \n",
    "\n",
    "**Documentation!** We need to add a module docstring and function docstrings. \n",
    "\n",
    "<br>\n",
    "\n",
    "⚙️ **Task:** Create a **module docstring** and **function docstrings** for each function. Refer to your notes and PEP 8 guidelines (also see `PEP 257`) to guide you on what these docstrings should contain. Edit the code below to make these changes. \n",
    "\n",
    ">⚠️ Do not change the logic of this code, only add documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import logging\n",
    "import pandas as pd\n",
    "# Name our logger so we know that logs from this module come from the data_ingestion module\n",
    "logger = logging.getLogger('data_ingestion')\n",
    "# Set a basic logging message up that prints out a timestamp, the name of our logger, and the message\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT *\n",
    "FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def create_db_engine(db_path):\n",
    "    try:\n",
    "        engine = create_engine(db_path)\n",
    "        # Test connection\n",
    "        with engine.connect() as conn:\n",
    "            pass\n",
    "        # test if the database engine was created successfully\n",
    "        logger.info(\"Database engine created successfully.\")\n",
    "        return engine # Return the engine object if it all works well\n",
    "    except ImportError: #If we get an ImportError, inform the user SQLAlchemy is not installed\n",
    "        logger.error(\"SQLAlchemy is required to use this function. Please install it first.\")\n",
    "        raise e\n",
    "    except Exception as e:# If we fail to create an engine inform the user\n",
    "        logger.error(f\"Failed to create database engine. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def query_data(engine, sql_query):\n",
    "    try:\n",
    "        with engine.connect() as connection:\n",
    "            df = pd.read_sql_query(text(sql_query), connection)\n",
    "        if df.empty:\n",
    "            # Log a message or handle the empty DataFrame scenario as needed\n",
    "            msg = \"The query returned an empty DataFrame.\"\n",
    "            logger.error(msg)\n",
    "            raise ValueError(msg)\n",
    "        logger.info(\"Query executed successfully.\")\n",
    "        return df\n",
    "    except ValueError as e: \n",
    "        logger.error(f\"SQL query failed. Error: {e}\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred while querying the database. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "def read_from_web_CSV(URL):\n",
    "    try:\n",
    "        df = pd.read_csv(URL)\n",
    "        logger.info(\"CSV file read successfully from the web.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logger.error(\"The URL does not point to a valid CSV file. Please check the URL and try again.\")\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read CSV from the web. Error: {e}\")\n",
    "        raise e\n",
    "    \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f0c4fb",
   "metadata": {},
   "source": [
    "Let's test our code to make sure it works before we move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27dcc193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 21:41:27,958 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 21:41:28,032 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 21:41:31,483 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-25 21:41:33,630 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "# Testing module functions  \n",
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)\n",
    "\n",
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6c609b",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "⚙️ **Task:** Once the `data_ingestion` code runs smoothly, create a new file, and name it `data_ingestion.py` and import the functions into the notebook. You will have to copy over the import statements and variables too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab292cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_ingestion\n",
      "data_ingestion\n",
      "data_ingestion\n"
     ]
    }
   ],
   "source": [
    "# Importing our new module\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "\n",
    "#Checking if the function names are now associated with the module\n",
    "print(create_db_engine.__module__)\n",
    "print(query_data.__module__)\n",
    "print(read_from_web_CSV.__module__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2fb109",
   "metadata": {},
   "source": [
    "\n",
    "Now the names `create_db_engine`, `query_data`, `read_from_web_CSV` are linked to the `data_ingestion` module, so our module is imported correctly. Lastly, we run the test commands again to make sure ite sure it works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87170695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 21:42:29,458 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 21:42:29,537 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 21:42:32,066 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-25 21:42:34,061 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field_df: (5654, 18), weather_df: (1843, 2), weather_mapping_df: (5654, 3)\n"
     ]
    }
   ],
   "source": [
    "field_df = query_data(create_db_engine(db_path), sql_query)   \n",
    "weather_df = read_from_web_CSV(weather_data_URL)\n",
    "weather_mapping_df = read_from_web_CSV(weather_mapping_data_URL)\n",
    "\n",
    "field_test = field_df.shape\n",
    "weather_test = weather_df.shape\n",
    "weather_mapping_test = weather_mapping_df.shape\n",
    "print(f\"field_df: {field_test}, weather_df: {weather_test}, weather_mapping_df: {weather_mapping_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabfe48",
   "metadata": {},
   "source": [
    "And there we go, we have a working data ingestion module! One down, two to go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d797db70",
   "metadata": {},
   "source": [
    "## Field data processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b43cc5",
   "metadata": {},
   "source": [
    "Next up, let's process the field data. This is the code we used last time to process the data, so let's start there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41eb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_agric_df = field_df.copy()\n",
    "\n",
    "MD_agric_df.rename(columns={'Annual_yield': 'Crop_type_Temp', 'Crop_type': 'Annual_yield'}, inplace=True)\n",
    "MD_agric_df.rename(columns={'Crop_type_Temp': 'Crop_type'}, inplace=True)\n",
    "MD_agric_df['Elevation'] = MD_agric_df['Elevation'].abs()\n",
    "\n",
    "# Correcting 'Crop_type' column\n",
    "def correct_crop_type(crop):\n",
    "    corrections = {\n",
    "        'cassaval': 'cassava',\n",
    "        'wheatn': 'wheat',\n",
    "        'teaa': 'tea'\n",
    "    }\n",
    "    return corrections.get(crop, crop)  # Get the corrected crop type, or return the original if not in corrections\n",
    "\n",
    "# Apply the correction function to the Crop_type column\n",
    "MD_agric_df['Crop_type'] = MD_agric_df['Crop_type'].apply(correct_crop_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770e25a2",
   "metadata": {},
   "source": [
    "Here our approach needs to be a bit different. If we create a module with a bunch of functions, we are going to have a hard time moving the DataFrame around the whole time. Instead, we're going to build a Class that encapsulates the whole data processing process for the field-related data called `FieldDataProcessor`. In the class, we will create a DataFrame attribute and methods that alter that attribute. So we encapsulate all of the logic in this `FieldDataProcessor` class, we abstract all of the details and only need to call something like `FieldDataProcessor.process_data()`. \n",
    "\n",
    "We could even include Inheritance and Polymorphism if we create a `DataProcessor` super class and create subclasses for `FieldDataProcessor` and `WeatherDataProcessor`.  But, there is a good reason not to. The data handling of the field data is quite different from the handling of the weather data. The field data comes from an SQL database, and we transform the data in a particular way, while the weather data is sourced from a CSV, and processed differently.\n",
    "\n",
    "So these two processes don't share processing steps, so it makes more sense to make a class for each.\n",
    "\n",
    "So let's create the class framework:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4179f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "        \n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class. \n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # DataFrame methods \n",
    "    def ingest_sql_data(self):\n",
    "        # First we want to get the data from the SQL database\n",
    "        pass\n",
    "    \n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order\n",
    "        \n",
    "        weather_map_df = self.weather_station_mapping() \n",
    "        self.df = self.ingest_sql_data()\n",
    "        self.df = self.rename_columns()\n",
    "        self.df = self.apply_corrections()\n",
    "        self.df = self.df.merge(weather_map_df, on='Field_ID', how='left')\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10660c3f",
   "metadata": {},
   "source": [
    "So this is the idea: We will instantiate the class, and call one method, `.process()` to ingest and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d2d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code won't run for now, since we have not defined all of the methods.\n",
    "field_processor = FieldDataProcessor()\n",
    "field_processor.process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e567c08",
   "metadata": {},
   "source": [
    "Now the notebook has a couple of lines of code but does an enormous amount of work to create the data pipeline. If we then call the Class `.df` attribute, we get the DataFrame, which we can analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe519e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_df = field_processor.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133df223",
   "metadata": {},
   "source": [
    "So all we have to do is create code for the various methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfabeb1",
   "metadata": {},
   "source": [
    "### `def ingest_sql_data()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b1aae",
   "metadata": {},
   "source": [
    "Let's create a copy of the class, and start filling out the code for the methods. We're dropping the `.process()` method for now and we'll add it back once it all works. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6108198f",
   "metadata": {},
   "source": [
    "⚙️ **Task:** Unscramble the code in the `.ingest_sql_data()` method. The method should return the initial DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "093fec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        return self.df\n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Annual_yield and Crop_type must be swapped\n",
    "        pass\n",
    "\n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d6352f",
   "metadata": {},
   "source": [
    "We can use the code below to check if the code works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b3f776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 21:54:55,306 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-25 21:54:55,309 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 21:54:55,377 - data_ingestion - INFO - Query executed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5654, 18)\n"
     ]
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_df = field_processor.df\n",
    "print(field_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c759a85",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "```python\n",
    "<Timestamp> - data_ingestion - INFO - Database engine created successfully.\n",
    "<Timestamp> - data_ingestion - INFO - Query executed successfully.\n",
    "<Timestamp> - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
    "(5654, 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7676579c",
   "metadata": {},
   "source": [
    "### `def rename_columns()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c11f136",
   "metadata": {},
   "source": [
    "Next up, we need to add `rename_columns()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47548fb7",
   "metadata": {},
   "source": [
    "⚙️ **Task:** Copy your class into the top of this cell, and unscramble the code sections in the `.rename_columns()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "369c190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "class FieldDataProcessor:\n",
    "\n",
    "    def __init__(self, logging_level=\"INFO\"): # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = 'sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "        \n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None \n",
    "        self.engine = None\n",
    "        \n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        return self.df\n",
    "        \n",
    "    def rename_columns(self):\n",
    "         # Extract the columns to rename from the configuration\n",
    "        column1, column2 = list(self.columns_to_rename.keys())[0], list(self.columns_to_rename.values())[0]  \n",
    "\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "        \n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "        \n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "\n",
    "            \n",
    "    def apply_corrections(self):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        pass\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28acacaf",
   "metadata": {},
   "source": [
    "Just note something here... `rename_columns()` does not return anything. It doesn't need to, because it is modifying the class attribute (data) `self.df`. This is the benefit of using a class. In each step we are applying changes to the DataFrame within the class. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de6c48",
   "metadata": {},
   "source": [
    "This code should instantiate the class, connect to the database, and swap the column names.\n",
    "\n",
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7233b3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 22:01:40,968 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-25 22:01:40,972 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 22:01:41,052 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 22:01:41,053 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.751354\n",
       "1    1.069865\n",
       "2    2.208801\n",
       "Name: Annual_yield, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_df = field_processor.df\n",
    "field_df['Annual_yield'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8461d0",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python 2024-02-13 14:35:56,581 - data_ingestion - INFO - Database engine created successfully.\n",
    "<Timestamp> - data_ingestion - INFO - Query executed successfully.\n",
    "<Timestamp> - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
    "<Timestamp> - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
    "0    0.751354\n",
    "1    1.069865\n",
    "2    2.208801\n",
    "Name: Annual_yield, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e74278",
   "metadata": {},
   "source": [
    "### `def apply_corrections()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f5dafe",
   "metadata": {},
   "source": [
    "⚙️ **Task:** Copy your class into the top of this cell, and fill in the `<MISSING CODE>` in the `.apply_corrections()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3065e60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "# Copy in your class including the ingest_sql_data method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    def __init__(\n",
    "        self, logging_level=\"INFO\"\n",
    "    ):  # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = \"sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db\"\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {\n",
    "            \"Annual_yield\": \"Crop_type\",\n",
    "            \"Crop_type\": \"Annual_yield\",\n",
    "        }\n",
    "        self.values_to_rename = {\n",
    "            \"cassaval\": \"cassava\",\n",
    "            \"wheatn\": \"wheat\",\n",
    "            \"teaa\": \"tea\",\n",
    "        }\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "\n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "            )\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        return self.df\n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = (\n",
    "            list(self.columns_to_rename.keys())[0],\n",
    "            list(self.columns_to_rename.values())[0],\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "\n",
    "\n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(lambda crop: self.values_to_rename.get(crop, crop))\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes, step by step. THis is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a92acc4",
   "metadata": {},
   "source": [
    "We can test if our new method works before we move onto the next one.\n",
    "\n",
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e33a085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 22:06:24,085 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-25 22:06:24,089 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 22:06:24,166 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 22:06:24,166 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Field_ID</th>\n",
       "      <th>Elevation</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Location</th>\n",
       "      <th>Slope</th>\n",
       "      <th>Rainfall</th>\n",
       "      <th>Min_temperature_C</th>\n",
       "      <th>Max_temperature_C</th>\n",
       "      <th>Ave_temps</th>\n",
       "      <th>Soil_fertility</th>\n",
       "      <th>Soil_type</th>\n",
       "      <th>pH</th>\n",
       "      <th>Pollution_level</th>\n",
       "      <th>Plot_size</th>\n",
       "      <th>Annual_yield</th>\n",
       "      <th>Crop_type</th>\n",
       "      <th>Standard_yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Field_ID, Elevation, Latitude, Longitude, Location, Slope, Rainfall, Min_temperature_C, Max_temperature_C, Ave_temps, Soil_fertility, Soil_type, pH, Pollution_level, Plot_size, Annual_yield, Crop_type, Standard_yield]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_processor.apply_corrections()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df.query(\"Crop_type in ['cassaval','wheatn']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aeacd6",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "Empty DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9794ee01",
   "metadata": {},
   "source": [
    "### `def weather_station_mapping()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46b624",
   "metadata": {},
   "source": [
    "⚙️ **Task:** Copy your class into the top of this cell, and fill in the `<MISSING CODE>` in the `.weather_station_mapping()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "29c58353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data and method here\n",
    "# Copy in your class including the ingest_sql_data method here\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "import logging\n",
    "from data_ingestion import read_from_web_CSV\n",
    "\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    def __init__(\n",
    "        self, logging_level=\"INFO\"\n",
    "    ):  # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = \"sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db\"\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {\n",
    "            \"Annual_yield\": \"Crop_type\",\n",
    "            \"Crop_type\": \"Annual_yield\",\n",
    "        }\n",
    "        self.values_to_rename = {\n",
    "            \"cassaval\": \"cassava\",\n",
    "            \"wheatn\": \"wheat\",\n",
    "            \"teaa\": \"tea\",\n",
    "        }\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "\n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "            )\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        return self.df\n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = (\n",
    "            list(self.columns_to_rename.keys())[0],\n",
    "            list(self.columns_to_rename.values())[0],\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "\n",
    "    def apply_corrections(self, column_name='Crop_type', abs_column='Elevation'):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(\n",
    "            lambda crop: self.values_to_rename.get(crop, crop)\n",
    "        )\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        self.df = read_from_web_CSV(self.weather_map_data)\n",
    "        return self.df\n",
    "    \n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. This is the method we will call, and it will call the other methods in order.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5458198a",
   "metadata": {},
   "source": [
    "Once again, test.\n",
    "\n",
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c05b721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 22:17:54,342 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-25 22:17:54,345 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 22:17:54,441 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 22:17:54,442 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-25 22:17:55,405 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.ingest_sql_data()\n",
    "field_processor.rename_columns()\n",
    "field_processor.apply_corrections()\n",
    "field_processor.weather_station_mapping()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0549ae",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python \n",
    "<Timestamp> - data_ingestion - INFO - Database engine created successfully.\n",
    "<Timestamp> - data_ingestion - INFO - Query executed successfully.\n",
    "<Timestamp> - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
    "<Timestamp> - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
    "<Timestamp> - data_ingestion - INFO - CSV file read successfully from the web.\n",
    "\n",
    "array([4, 0, 1, 2, 3], dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552da32c",
   "metadata": {},
   "source": [
    "### `def process()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37328e56",
   "metadata": {},
   "source": [
    "Ok, now we put it all together. Remember that the `.process()` method calls all of the other methods in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdfc50",
   "metadata": {},
   "source": [
    "⚙️ **Task:** Copy your class into the top of this cell, and complete the `.process()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e526e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy in your class including the ingest_sql_data and  method here\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    def __init__(\n",
    "        self, logging_level=\"INFO\"\n",
    "    ):  # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        # Initialising class with attributes we need. Refer to the code above to understand how each attribute relates to the code\n",
    "        self.db_path = \"sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db\"\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {\n",
    "            \"Annual_yield\": \"Crop_type\",\n",
    "            \"Crop_type\": \"Annual_yield\",\n",
    "        }\n",
    "        self.values_to_rename = {\n",
    "            \"cassaval\": \"cassava\",\n",
    "            \"wheatn\": \"wheat\",\n",
    "            \"teaa\": \"tea\",\n",
    "        }\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "\n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "            )\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        return self.df\n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = (\n",
    "            list(self.columns_to_rename.keys())[0],\n",
    "            list(self.columns_to_rename.values())[0],\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "\n",
    "    def apply_corrections(\n",
    "        self, column_name=\"Crop_type\", abs_column=\"Elevation\"\n",
    "    ):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(\n",
    "            lambda crop: self.values_to_rename.get(crop, crop)\n",
    "        )\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        return read_from_web_CSV(self.weather_map_data)\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods and applies the changes, step by step. \n",
    "        # This is the method we will call, and it will call the other methods in order\n",
    "        self.ingest_sql_data()\n",
    "        self.apply_corrections()\n",
    "        self.rename_columns()\n",
    "        weather_map_df = self.weather_station_mapping() \n",
    "        self.df = self.df.merge(weather_map_df, on='Field_ID', how='left')\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e6a06a",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3645f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 10:23:10,106 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 10:23:10,169 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 10:23:10,170 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-26 10:23:10,174 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 10:23:10,670 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor()\n",
    "field_processor.process()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0108536",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "<Timestamp>  - data_ingestion - INFO - Database engine created successfully.\n",
    "<Timestamp>  - data_ingestion - INFO - Query executed successfully.\n",
    "<Timestamp>  - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
    "<Timestamp>  - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
    "<Timestamp> - data_ingestion - INFO - CSV file read successfully from the web.\n",
    "\n",
    "array([4, 0, 1, 2, 3], dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70083d45",
   "metadata": {},
   "source": [
    "### Centralising the data pipeline configuration details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5380c7",
   "metadata": {},
   "source": [
    "But now we're running into a problem. We stored some data about the SQL database and web files in the `data_ingestion.py` module, and when we load the `field_data_proccessor.py` module we are referencing it again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b70f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the data_ingestion.py module\n",
    "\n",
    "db_path = 'sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\"\n",
    "\n",
    "\n",
    "# From the field_data_processor class\n",
    "        self.db_path = 'sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db'\n",
    "        self.sql_query = \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\"\n",
    "        self.columns_to_rename = {'Annual_yield': 'Crop_type', 'Crop_type': 'Annual_yield'}\n",
    "        self.values_to_rename = {'cassaval': 'cassava', 'wheatn': 'wheat', 'teaa': 'tea'}\n",
    "        self.weather_map_data = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6daf3a72",
   "metadata": {},
   "source": [
    "So if we want to change the database path, the query, or anything else, where would we do it? It would be better to have one central place where we store all of the specific details of the data pipeline and refer to it in the modules and our main script. A good approach is to create a dictionary in our main script that has all of the parameters, and then we reference it in our modules. \n",
    "\n",
    "<br>\n",
    "\n",
    "⚙️ **Task:** Add the configuration details from the `data_ingestion.py` module into the `config_params` dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec9640",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params = {\n",
    "    \"sql_query\": \"\"\"\n",
    "        \n",
    "            \"\"\", # Insert your SQL query\n",
    "    \"db_path\": , # Insert the db_path of the database\n",
    "    \"columns_to_rename\": # Insert the disctionary of columns we want to swop the names of, \n",
    "    \"values_to_rename\": , # Insert the croptype renaming dictionary\n",
    "    \"weather_csv_path\": , # Insert the weather data CSV here\n",
    "    \"weather_mapping_csv\":, # Insert the weather data mapping CSV here\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b794548",
   "metadata": {},
   "source": [
    "Now we can remove these lines form the `data_ingestion.py` module file, since we call them from the `FieldDataProcessor` class.\n",
    "\n",
    "<br>\n",
    "\n",
    "⚙️ **Task:** Remove the following lines from the `data_ingestion.py` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0922e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these lines from the data_ingestion.py module\n",
    "db_path = 'sqlite:///Maji_Ndogo_farm_survey_small.db'\n",
    "\n",
    "sql_query = \"\"\"\n",
    "SELECT * FROM geographic_features\n",
    "LEFT JOIN weather_features USING (Field_ID)\n",
    "LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "LEFT JOIN farm_management_features USING (Field_ID)\n",
    "\"\"\"\n",
    "\n",
    "weather_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\"\n",
    "weather_mapping_data_URL = \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5982650b",
   "metadata": {},
   "source": [
    "In the `FieldDataProcessor` class, instead of passing in the parameters as strings, we reference the `config_params` dictionary instead.\n",
    "\n",
    "<br>\n",
    "\n",
    "⚙️ **Task:** Alter the attributes of the `FieldDataProcessor` class to reference the `config_params` dictionary instead. Add `config_params` as a parameter to the class instantiation method as shown below and complete the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5c62c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "# Copy in your class including the ingest_sql_data method here\n",
    "import logging\n",
    "\n",
    "import pandas as pd\n",
    "from data_ingestion import create_db_engine, query_data, read_from_web_CSV\n",
    "\n",
    "config_params = {\n",
    "    \"db_path\": \"sqlite:///Student_pack/Maji_Ndogo_farm_survey_small.db\",\n",
    "    \"sql_query\": \"\"\"SELECT *\n",
    "            FROM geographic_features\n",
    "            LEFT JOIN weather_features USING (Field_ID)\n",
    "            LEFT JOIN soil_and_crop_features USING (Field_ID)\n",
    "            LEFT JOIN farm_management_features USING (Field_ID)\n",
    "            \"\"\",\n",
    "    \"columns_to_rename\": {\n",
    "        \"Annual_yield\": \"Crop_type\",\n",
    "        \"Crop_type\": \"Annual_yield\",\n",
    "    },\n",
    "    \"values_to_rename\": {\n",
    "        \"cassaval\": \"cassava\",\n",
    "        \"wheatn\": \"wheat\",\n",
    "        \"teaa\": \"tea\",\n",
    "    },\n",
    "    \"weather_map_data\": \"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "class FieldDataProcessor:\n",
    "    def __init__(\n",
    "        self, config_params, logging_level=\"INFO\"\n",
    "    ):  # When we instantiate this class, we can optionally specify what logs we want to see\n",
    "        # Initialising class with attributes we need. Refer to the code above\n",
    "        # to understand how each attribute relates to the code\n",
    "        self.db_path = config_params[\"db_path\"]\n",
    "        self.sql_query = config_params[\"sql_query\"]\n",
    "        self.columns_to_rename = config_params[\"columns_to_rename\"]\n",
    "        self.values_to_rename = config_params[\"values_to_rename\"]\n",
    "        self.weather_map_data = config_params[\"weather_map_data\"]\n",
    "\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "        # We create empty objects to store the DataFrame and engine in\n",
    "        self.df = None\n",
    "        self.engine = None\n",
    "\n",
    "    # This method enables logging in the class.\n",
    "    def initialize_logging(self, logging_level):\n",
    "        \"\"\"\n",
    "        Sets up logging for this instance of FieldDataProcessor.\n",
    "        \"\"\"\n",
    "        logger_name = __name__ + \".FieldDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter(\n",
    "                \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "            )\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "        # Use self.logger.info(), self.logger.debug(), etc.\n",
    "\n",
    "    # let's focus only on this part from now on\n",
    "    def ingest_sql_data(self):\n",
    "        self.logger.info(\"Sucessfully loaded data.\")\n",
    "        self.engine = create_db_engine(self.db_path)\n",
    "        self.df = query_data(self.engine, self.sql_query)\n",
    "        return self.df\n",
    "\n",
    "    def rename_columns(self):\n",
    "        # Extract the columns to rename from the configuration\n",
    "        column1, column2 = (\n",
    "            list(self.columns_to_rename.keys())[0],\n",
    "            list(self.columns_to_rename.values())[0],\n",
    "        )\n",
    "\n",
    "        self.logger.info(f\"Swapped columns: {column1} with {column2}\")\n",
    "\n",
    "        # Temporarily rename one of the columns to avoid a naming conflict\n",
    "        temp_name = \"__temp_name_for_swap__\"\n",
    "        while temp_name in self.df.columns:\n",
    "            temp_name += \"_\"\n",
    "\n",
    "        # Perform the swap\n",
    "        self.df = self.df.rename(columns={column1: temp_name, column2: column1})\n",
    "        self.df = self.df.rename(columns={temp_name: column2})\n",
    "\n",
    "    def apply_corrections(\n",
    "        self, column_name=\"Crop_type\", abs_column=\"Elevation\"\n",
    "    ):\n",
    "        # Correct the crop strings, Eg: 'cassaval' -> 'cassava'\n",
    "        self.df[abs_column] = self.df[abs_column].abs()\n",
    "        self.df[column_name] = self.df[column_name].apply(\n",
    "            lambda crop: self.values_to_rename.get(crop, crop)\n",
    "        )\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        # Merge the weather station data to the main DataFrame\n",
    "        return read_from_web_CSV(self.weather_map_data)\n",
    "\n",
    "    def process(self):\n",
    "        # This process calls the correct methods, and applies the changes,\n",
    "        # step by step. THis is the method we will call, and it will call\n",
    "        # the other methods in order.\n",
    "        self.df = self.ingest_sql_data()\n",
    "        self.rename_columns()\n",
    "        self.apply_corrections()\n",
    "        weather_map_df = self.weather_station_mapping()\n",
    "        self.df = self.df.merge(weather_map_df, on=\"Field_ID\", how=\"left\")\n",
    "        self.df = self.df.drop(columns=\"Unnamed: 0\")\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae0605f",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "⚙️ **Task:** Instantiate the class with the new dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd00ec",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8712fd3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 23:08:04,431 - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-25 23:08:04,434 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 23:08:04,502 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 23:08:04,502 - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-25 23:08:04,935 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "\n",
    "field_df = field_processor.df\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9898937",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "<Timestamp>  - data_ingestion - INFO - Database engine created successfully.\n",
    "<Timestamp>  - data_ingestion - INFO - Query executed successfully.\n",
    "<Timestamp>  - __main__.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
    "<Timestamp>  - __main__.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
    "<Timestamp> - data_ingestion - INFO - CSV file read successfully from the web.\n",
    "\n",
    "array([4, 0, 1, 2, 3], dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f595162",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "### Creating `field_data_processor.py`\n",
    "\n",
    "Now we have a robust data processing class for the field-related data. Our final step is to create the module file and document our code.\n",
    "\n",
    "<br>\n",
    "\n",
    "⚙️ **Task:** Complete the `field_data_processor` module. Include all of the required content, ensure the module is PEP 8 complient, include all imports and parameter definitions, and create the `field_data_processor.py` module file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1679c0",
   "metadata": {},
   "source": [
    "Restart the kernel before running this code:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f4b44",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bcad5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 23:14:20,310 - field_data_processor.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-25 23:14:20,313 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-25 23:14:20,395 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-25 23:14:20,395 - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-25 23:14:20,846 - data_ingestion - INFO - CSV file read successfully from the web.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 3])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re # Importing all the packages we will use eventually\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor, config_params\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = config_params # Paste in your config_params dictionary here\n",
    "\n",
    "\n",
    "# Instantiating the class with config_params passed to the class as a parameter \n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "field_df['Weather_station'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47944227",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "<Timestamp>  - data_ingestion - INFO - Database engine created successfully.\n",
    "<Timestamp>  - data_ingestion - INFO - Query executed successfully.\n",
    "<Timestamp>  - field_data_processor.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
    "<Timestamp>  - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
    "<Timestamp> - data_ingestion - INFO - CSV file read successfully from the web.\n",
    "\n",
    "array([4, 0, 1, 2, 3], dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04508d",
   "metadata": {},
   "source": [
    "## Weather data processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a595d0",
   "metadata": {},
   "source": [
    "Now for the last module. The `WeatherDataProcessor` class will be dealing with all of the weather-related data. Again we want to instantiate the class, then call a `.process()` method to import and clean the data. Here is the code we used last time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af63063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Importing the regex pattern\n",
    "import numpy as np\n",
    "\n",
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")\n",
    "\n",
    "patterns = {\n",
    "    'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "     'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "    'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "}\n",
    "\n",
    "def extract_measurement(message):\n",
    "    \"\"\"\n",
    "    Extracts a numeric measurement value from a given message string.\n",
    "\n",
    "    The function applies regular expressions to identify and extract\n",
    "    numeric values related to different types of measurements such as\n",
    "    Rainfall, Average Temperatures, and Pollution Levels from a text message.\n",
    "    It returns the key of the matching record, and first matching value as a floating-point number.\n",
    "    \n",
    "    Parameters:\n",
    "    message (str): A string message containing the measurement information.\n",
    "\n",
    "    Returns:\n",
    "    float: The extracted numeric value of the measurement if a match is found;\n",
    "           otherwise, None.\n",
    "\n",
    "    The function uses the following patterns for extraction:\n",
    "    - Rainfall: Matches numbers (including decimal) followed by 'mm', optionally spaced.\n",
    "    - Ave_temps: Matches numbers (including decimal) followed by 'C', optionally spaced.\n",
    "    - Pollution_level: Matches numbers (including decimal) following 'Pollution at' or '='.\n",
    "    \n",
    "    Example usage:\n",
    "    extract_measurement(\"【2022-01-04 21:47:48】温度感应: 现在温度是 12.82C.\")\n",
    "    # Returns: 'Temperature', 12.82\n",
    "    \"\"\"\n",
    "    \n",
    "    for key, pattern in patterns.items(): # Loop through all of the patterns and check if it matches the pattern value.\n",
    "        match = re.search(pattern, message)\n",
    "        if match:\n",
    "            # Extract the first group that matches, which should be the measurement value if all previous matches are empty.\n",
    "            # print(match.groups()) # Uncomment this line to help you debug your regex patterns.\n",
    "            return key, float(next((x for x in match.groups() if x is not None)))\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n",
    "\n",
    "# The function creates a tuple with the measurement type and value into a Pandas Series\n",
    "result = weather_station_df['Message'].apply(extract_measurement)\n",
    "\n",
    "# Create separate columns for 'Measurement' and 'extracted_value' by unpacking the tuple with Lambda functions.\n",
    "weather_station_df['Measurement'] = result.apply(lambda x: x[0])\n",
    "weather_station_df['Value'] = result.apply(lambda x: x[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2937ec",
   "metadata": {},
   "source": [
    "Luckily the other team did most of the work this time, so we only have to fill in a couple of details. First off, we need to add more keys to the `config_params` dictionary. Specifically the regex pattern we used to get the messages, and the URL of the weather data.\n",
    "\n",
    "<br>\n",
    "\n",
    "⚙️ **Task:** Complete the values for the new keys in `config_params`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a30fc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_station_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\")\n",
    "weather_station_mapping_df = pd.read_csv(\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_data_field_mapping.csv\")\n",
    "\n",
    "    patterns = {\n",
    "        'Rainfall': r'(\\d+(\\.\\d+)?)\\s?mm',\n",
    "        'Temperature': r'(\\d+(\\.\\d+)?)\\s?C',\n",
    "        'Pollution_level': r'=\\s*(-?\\d+(\\.\\d+)?)|Pollution at \\s*(-?\\d+(\\.\\d+)?)'\n",
    "    }\n",
    "    \n",
    "    config_params = {\n",
    "       \"weather_csv_path\":\"https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Maji_Ndogo/Weather_station_data.csv\", # Insert the URL for the weather station data\n",
    "       \"regex_patterns\": patterns,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d1d93",
   "metadata": {},
   "source": [
    "Then we already have the class fully set up. We just have to make sure the formatting is correct, and that the module is documented properly.\n",
    "\n",
    "⚙️ **Task:** Complete the `weather_data_processor` module. Include all of the required content, ensure the module is PEP 8 compliant, include all imports and parameter definitions, and create the `weather_data_processor.py` module file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f56158be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the imports we're going to use in the weather data processing module\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "from data_ingestion import read_from_web_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "230dfb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION \n",
    "\n",
    "class WeatherDataProcessor:\n",
    "    def __init__(self, config_params, logging_level=\"INFO\"): # Now we're passing in the confi_params dictionary already\n",
    "        self.weather_station_data = config_params['weather_csv_path']\n",
    "        self.patterns = config_params['regex_patterns']\n",
    "        self.weather_df = None  # Initialize weather_df as None or as an empty DataFrame\n",
    "        self.initialize_logging(logging_level)\n",
    "\n",
    "    def initialize_logging(self, logging_level):\n",
    "        logger_name = __name__ + \".WeatherDataProcessor\"\n",
    "        self.logger = logging.getLogger(logger_name)\n",
    "        self.logger.propagate = False  # Prevents log messages from being propagated to the root logger\n",
    "\n",
    "        # Set logging level\n",
    "        if logging_level.upper() == \"DEBUG\":\n",
    "            log_level = logging.DEBUG\n",
    "        elif logging_level.upper() == \"INFO\":\n",
    "            log_level = logging.INFO\n",
    "        elif logging_level.upper() == \"NONE\":  # Option to disable logging\n",
    "            self.logger.disabled = True\n",
    "            return\n",
    "        else:\n",
    "            log_level = logging.INFO  # Default to INFO\n",
    "\n",
    "        self.logger.setLevel(log_level)\n",
    "\n",
    "        # Only add handler if not already added to avoid duplicate messages\n",
    "        if not self.logger.handlers:\n",
    "            ch = logging.StreamHandler()  # Create console handler\n",
    "            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "            ch.setFormatter(formatter)\n",
    "            self.logger.addHandler(ch)\n",
    "\n",
    "    def weather_station_mapping(self):\n",
    "        self.weather_df = read_from_web_CSV(self.weather_station_data)\n",
    "        self.logger.info(\"Successfully loaded weather station data from the web.\") \n",
    "        # Here, you can apply any initial transformations to self.weather_df if necessary.\n",
    "\n",
    "    \n",
    "    def extract_measurement(self, message):\n",
    "        for key, pattern in self.patterns.items():\n",
    "            match = re.search(pattern, message)\n",
    "            if match:\n",
    "                self.logger.debug(f\"Measurement extracted: {key}\")\n",
    "                return key, float(next((x for x in match.groups() if x is not None)))\n",
    "        self.logger.debug(\"No measurement match found.\")\n",
    "        return None, None\n",
    "\n",
    "    def process_messages(self):\n",
    "        if self.weather_df is not None:\n",
    "            result = self.weather_df['Message'].apply(self.extract_measurement)\n",
    "            self.weather_df['Measurement'], self.weather_df['Value'] = zip(*result)\n",
    "            self.logger.info(\"Messages processed and measurements extracted.\")\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, skipping message processing.\")\n",
    "        return self.weather_df\n",
    "\n",
    "    def calculate_means(self):\n",
    "        if self.weather_df is not None:\n",
    "            means = self.weather_df.groupby(by=['Weather_station_ID', 'Measurement'])['Value'].mean()\n",
    "            self.logger.info(\"Mean values calculated.\")\n",
    "            return means.unstack()\n",
    "        else:\n",
    "            self.logger.warning(\"weather_df is not initialized, cannot calculate means.\")\n",
    "            return None\n",
    "    \n",
    "    def process(self):\n",
    "        self.weather_station_mapping()  # Load and assign data to weather_df\n",
    "        self.process_messages()  # Process messages to extract measurements\n",
    "        self.logger.info(\"Data processing completed.\")\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71179f10",
   "metadata": {},
   "source": [
    "Once we have the `weather_data_processor` module set up, we can run the code below to import the new module, and make sure our module worked correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72163fa3",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4af300e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 01:06:27,984 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 01:06:27,986 - weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-02-26 01:06:28,025 - weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-02-26 01:06:28,026 - weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Temperature', 'Pollution_level', 'Rainfall'], dtype=object)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from field_data_processor import FieldDataProcessor\n",
    "from weather_data_processor import WeatherDataProcessor, config_params\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params = config_params # Paste in your config_params dictionary here\n",
    "\n",
    "# Ignoring the field data for now.\n",
    "# field_processor = FieldDataProcessor(config_params)\n",
    "# field_processor.process()\n",
    "# field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(config_params)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "weather_df['Measurement'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e121864",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "<Timestamp> - data_ingestion - INFO - CSV file read successfully from the web.\n",
    "<Timestamp> - __main__.WeatherDataProcessor  - INFO - Successfully loaded weather station data from the web.\n",
    "<Timestamp> - __main__.WeatherDataProcessor  - INFO - Messages processed and measurements extracted.\n",
    "<Timestamp> - __main__.WeatherDataProcessor  - INFO - Data processing completed.\n",
    "\n",
    "array(['Temperature', 'Pollution_level', 'Rainfall'], dtype=object)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d54794",
   "metadata": {},
   "source": [
    "### Validating our data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a9dec5",
   "metadata": {},
   "source": [
    "So we finally have working modules that now automatically pull data from the database  (or the web), process it, clean it, and return our starting DataFrame. Before we jump in and analyse the data, let's pause for a second and ask: Did the changes actually get applied? Did we correct the elevation data, did we rename the columns? We could go back to the old ways, and create queries to check, but a better way is to **test our dataset**. \n",
    "\n",
    "Let's get the data in first. Remember to use your `config_params` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0350663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 09:01:50,684 - field_data_processor.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-26 09:01:50,685 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 09:01:50,728 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 09:01:50,729 - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 09:01:51,583 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 09:01:52,166 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 09:01:52,166 - weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-02-26 09:01:52,185 - weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-02-26 09:01:52,186 - weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor, config_params as field_config\n",
    "from weather_data_processor import WeatherDataProcessor, config_params as weather_config\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "field_processor = FieldDataProcessor(field_config)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(weather_config)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611d171",
   "metadata": {},
   "source": [
    "There should be a `validate_data.py` file in the notebook directory. This is a `pytest` script that does a couple of tests to see if the data we're expecting, is what we actually have. Have a look at the test script, and try to understand what we're testing.\n",
    "\n",
    "`pytest` normally runs from the command line because it is set up to be automated. To test the data, we have to give `pytest` access to that data. The simplest way to do this is by creating CSV files, importing them into `validate_data.py`, and running the tests.\n",
    "\n",
    "The following code creates CSV files, runs `pytest` in the terminal using `!pytest validate_data.py -v`, and deletes the CSV files once the test is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5456e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mpytest\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.7, pytest-8.0.2, pluggy-1.4.0 -- /home/ultracode/.cache/pypoetry/virtualenvs/datascience-Hp-DJPih-py3.11/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/ultracode/repos/ds/src/Python\n",
      "plugins: anyio-4.2.0\n",
      "collected 0 items                                                              \u001b[0m\n",
      "\n",
      "\u001b[33m============================ \u001b[33mno tests ran\u001b[0m\u001b[33m in 0.00s\u001b[0m\u001b[33m =============================\u001b[0m\n",
      "\u001b[31mERROR: file or directory not found: validate_data.py\n",
      "\u001b[0m\n",
      "Exception: \u001b[31;1mpytest exited with 4\u001b[m\n",
      "  code from -c:1:1-26: \u001b[1;4mpytest validate_data.py -v\u001b[m\n",
      "Deleted sampled_weather_df.csv\n",
      "Deleted sampled_field_df.csv\n"
     ]
    }
   ],
   "source": [
    "#!poetry add pytest\n",
    "\n",
    "weather_df.to_csv('sampled_weather_df.csv', index=False)\n",
    "field_df.to_csv('sampled_field_df.csv', index=False)\n",
    "\n",
    "!pytest validate_data.py -v\n",
    "\n",
    "import os# Define the file paths\n",
    "weather_csv_path = 'sampled_weather_df.csv'\n",
    "field_csv_path = 'sampled_field_df.csv'\n",
    "\n",
    "# Delete sampled_weather_df.csv if it exists\n",
    "if os.path.exists(weather_csv_path):\n",
    "    os.remove(weather_csv_path)\n",
    "    print(f\"Deleted {weather_csv_path}\")\n",
    "else:\n",
    "    print(f\"{weather_csv_path} does not exist.\")\n",
    "\n",
    "# Delete sampled_field_df.csv if it exists\n",
    "if os.path.exists(field_csv_path):\n",
    "    os.remove(field_csv_path)\n",
    "    print(f\"Deleted {field_csv_path}\")\n",
    "else:\n",
    "    print(f\"{field_csv_path} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab08492",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "```python\n",
    "============================ test session starts =============================\n",
    "platform win32 -- Python 3.12.1, pytest-8.0.0, pluggy-1.4.0 -- ...\n",
    "cachedir: .pytest_cache\n",
    "rootdir: ...\n",
    "plugins: anyio-4.2.0\n",
    "collecting ... collected 7 items\n",
    "\n",
    "validate_data.py::test_read_weather_DataFrame_shape PASSED               [ 14%]\n",
    "validate_data.py::test_read_field_DataFrame_shape PASSED                 [ 28%]\n",
    "validate_data.py::test_weather_DataFrame_columns PASSED                  [ 42%]\n",
    "validate_data.py::test_field_DataFrame_columns PASSED                    [ 57%]\n",
    "validate_data.py::test_field_DataFrame_non_negative_elevation PASSED     [ 71%]\n",
    "validate_data.py::test_crop_types_are_valid PASSED                       [ 85%]\n",
    "validate_data.py::test_positive_rainfall_values PASSED                   [100%]\n",
    "\n",
    "============================== warnings summary ===============================\n",
    "..\\..\\..\\..\\..\\..\\..\\..\\anaconda3\\envs\\Latest\\Lib\\site-packages\\dateutil\\tz\\tz.py:37\n",
    "  ...: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
    "    EPOCH = datetime.datetime.utcfromtimestamp(0)\n",
    "\n",
    "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
    "======================== 7 passed, 1 warning in 1.13s =========================\n",
    "Deleted sampled_weather_df.csv\n",
    "Deleted sampled_field_df.csv\n",
    "```\n",
    "\n",
    ">⚠️ Depending on the version of Python, there may be various warnings like the one above. These are normally `DeprecationWarnings` so we can safely ignore these for now. We're interested in whether all the dataset tests passed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e09daa",
   "metadata": {},
   "source": [
    "Great! Now we know our data resembles what we expect! As our project evolves we may have to add more module functionality or create more rigorous tests of the data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca09816",
   "metadata": {},
   "source": [
    "# Validating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7c6cc",
   "metadata": {},
   "source": [
    "Before we actually get to the analysis part, take a moment to notice how much simpler this data import is now. It feels like a lot of work, but now one cell of code imports and cleans all of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5c5ed490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-26 01:23:36,631 - field_data_processor.FieldDataProcessor - INFO - Sucessfully loaded data.\n",
      "2024-02-26 01:23:36,637 - data_ingestion - INFO - Database engine created successfully.\n",
      "2024-02-26 01:23:36,944 - data_ingestion - INFO - Query executed successfully.\n",
      "2024-02-26 01:23:36,946 - field_data_processor.FieldDataProcessor - INFO - Swapped columns: Annual_yield with Crop_type\n",
      "2024-02-26 01:23:37,325 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 01:23:37,778 - data_ingestion - INFO - CSV file read successfully from the web.\n",
      "2024-02-26 01:23:37,784 - weather_data_processor.WeatherDataProcessor - INFO - Successfully loaded weather station data from the web.\n",
      "2024-02-26 01:23:37,905 - weather_data_processor.WeatherDataProcessor - INFO - Messages processed and measurements extracted.\n",
      "2024-02-26 01:23:37,908 - weather_data_processor.WeatherDataProcessor - INFO - Data processing completed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from field_data_processor import FieldDataProcessor, config_params as field_config\n",
    "from weather_data_processor import WeatherDataProcessor, config_params as weather_config\n",
    "import logging \n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "config_params =  field_config  # Paste in your previous dictionary data in here\n",
    "\n",
    "field_processor = FieldDataProcessor(config_params)\n",
    "field_processor.process()\n",
    "field_df = field_processor.df\n",
    "\n",
    "weather_processor = WeatherDataProcessor(weather_config)\n",
    "weather_processor.process()\n",
    "weather_df = weather_processor.weather_df\n",
    "\n",
    "# Rename 'Ave_temps' in field_df to 'Temperature' to match weather_df\n",
    "field_df.rename(columns={'Ave_temps': 'Temperature'}, inplace=True)\n",
    "#field_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f7e9d8",
   "metadata": {},
   "source": [
    "Ok, now we can circle back to the start. As I mentioned, setting a tolerance might have been a simple way to measure if our field data and weather data agree, but we didn't take into account if either dataset was spread out.\n",
    "\n",
    "I hope you have some idea of the problem, but we need to tell this story anyway, so hopefully, I can convince you I made an error last time, by the time we're done.\n",
    "\n",
    "Back to our initial plan:\n",
    "\n",
    "1. Create a null hypothesis.\n",
    "1. Import the `MD_agric_df` dataset and clean it up.\n",
    "1. Import the weather data.\n",
    "1. Map the weather data to the field data.\n",
    "1. Calculate the means of the weather station dataset and the means of the main dataset.\n",
    "2. Calculate all the parameters we need to do a t-test. \n",
    "3. Interpret our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbcc03",
   "metadata": {},
   "source": [
    "## Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b650117",
   "metadata": {},
   "source": [
    "So what are we testing with our null hypothesis $H_0$? Well, we want to know if our field data is representing the reality in Maji Ndogo by looking at an independent set of data. If our field data (means) are the same as the weather data (means), then it indicates no significant difference between the datasets. We're essentially saying that any difference we see between these means is because of randomness. However, if the means differ significantly, we'll know there is a reason for it, and that it is not just a random fluctuation in the data. \n",
    "\n",
    "<br>\n",
    "\n",
    "Given a significance level $\\alpha$ of 0.05 for a two-tailed test, we have the following conditions for our hypothesis test at a 95% confidence interval:\n",
    "\n",
    "- $H_0$: There is no significant difference between the means of the two datasets. This is expressed as $\\mu_{field} = \\mu_{weather}$.\n",
    "\n",
    "- $H_a$: There is a significant difference between the means of the two datasets. This is expressed as $\\mu_{field} \\neq \\mu_{weather}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "If the p-value obtained from the test:\n",
    "- is less than or equal to the significance level, so $p \\leq \\alpha$, we reject the null hypothesis.\n",
    "- is larger than the significance level, so $p > \\alpha$, we cannot reject the null hypothesis, as we cannot find a statistically significant difference between the datasets at the 95% confidence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1bf54",
   "metadata": {},
   "source": [
    "Now, let's code it out. \n",
    "\n",
    "First, we're going to import all of the packages and define a few variables. You might notice we're importing a new method, `.ttest_ind()`. This method takes in two data columns and calculates means, variance, and returns the the t- and p-statistics. So our t-test is reduced to one line. Since our alternative hypothesis does not make a claim of greater or less than, we will use the two-sided t-test, by adding  the `alternative = 'two-sided'` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5833b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following packages are already present in the pyproject.toml and will be skipped:\n",
      "\n",
      "  • \u001b[36mscipy\u001b[39m\n",
      "\n",
      "If you want to update it to the latest compatible version, you can use `poetry update package`.\n",
      "If you prefer to upgrade it to the latest available version, you can use `poetry add package@latest`.\n",
      "\n",
      "Nothing to add.\n"
     ]
    }
   ],
   "source": [
    "#!poetry add scipy\n",
    "from scipy.stats import ttest_ind\n",
    "import numpy as np\n",
    "\n",
    "# Now, the measurements_to_compare can directly use 'Temperature', 'Rainfall', and 'Pollution_level'\n",
    "measurements_to_compare = ['Temperature', 'Rainfall', 'Pollution_level']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02f085a",
   "metadata": {},
   "source": [
    "Let's pause for a second and clarify what exactly we're comparing. \n",
    "\n",
    "We want to compare the means of the temperature, rainfall, and pollution data, for fields assigned to a specific weather station. So for both datasets, we need to isolate the measurement type and weather station for each data, so we're comparing the correct means.\n",
    "\n",
    "Let's break down what we need to do:\n",
    "1. We need to filter both `field_df` and `weather_df` based on the given station ID and measurement. We can use `filter_field_data(df, station_id, measurement)` and `filter_weather_data(df, station_id, measurement)`.  \n",
    "2. We need to perform a t-test to conduct the t-test on the filtered data. So we're going to use `ttest_ind(data_col1, data_col2, equal_var=False)` from `scipy.stats`.\n",
    "3. `print_ttest_results(station_id, measurement, p_val, alpha)` to interpret and print the results from the t-test.\n",
    "\n",
    "We'll first define these functions, focusing on `Temperature` for `station ID = 0`. Then, we'll integrate these functions into a loop that iterates over each station ID and measurement type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe83fc",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "⚙️ **Task:** Create a `filter_field_data` function that takes in the `field_df` DataFrame, the `station_id`, and `measurement` type, and retuns a **single column** (series) of data filtered by the `station_id`, and `measurement`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f2b52be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def filter_field_data(df, station_id, measurement):\n",
    "    df = df[measurement][df['Weather_station']== station_id]\n",
    "    return df\n",
    "    \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be8aba",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Input 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "123ae3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       13.35\n",
      "2       13.30\n",
      "8       12.80\n",
      "10      13.70\n",
      "14      13.35\n",
      "        ...  \n",
      "5627    13.30\n",
      "5630    14.25\n",
      "5632    11.00\n",
      "5638    13.30\n",
      "5642    12.85\n",
      "Name: Temperature, Length: 1375, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "#field_df\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "print(field_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1723c0",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "\n",
    "```python\n",
    "1       13.35\n",
    "2       13.30\n",
    "8       12.80\n",
    "10      13.70\n",
    "14      13.35\n",
    "        ...  \n",
    "5627    13.30\n",
    "5630    14.25\n",
    "5632    11.00\n",
    "5638    13.30\n",
    "5642    12.85\n",
    "Name: Temperature, Length: 1375, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb677fc4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Input 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "99f9178a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1375,), First value: 13.35 \n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "print(f\"Shape: {field_values.shape}, First value: {field_values.iloc[0]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb2a378",
   "metadata": {},
   "source": [
    "**Expected outcome:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfff140a",
   "metadata": {},
   "source": [
    "`Shape: (1375,), First value: 13.35 `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ac4c9",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "⚙️ **Task:** Create a data filter function that takes in the `weather_df` DataFrame, the `station_id`, and `measurement` type, and returns a **single column** (series) of data filtered by the `station_id`, and `measurement`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "50110261",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def filter_weather_data(df, station_id, measurement):\n",
    "    df = df[(df[\"Measurement\"]==measurement) & (df['Weather_station_ID']==station_id)]['Value']\n",
    "    return df\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d919e41",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "**Input 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a948c941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       12.82\n",
       "2       14.53\n",
       "29      14.28\n",
       "32      12.87\n",
       "67      13.13\n",
       "        ...  \n",
       "1804    12.77\n",
       "1805    14.13\n",
       "1817    13.14\n",
       "1833    14.14\n",
       "1834    13.61\n",
       "Name: Value, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "weather_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2092f164",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "\n",
    "```python\n",
    "0       12.82\n",
    "2       14.53\n",
    "29      14.28\n",
    "32      12.87\n",
    "67      13.13\n",
    "        ...  \n",
    "1804    12.77\n",
    "1805    14.13\n",
    "1817    13.14\n",
    "1833    14.14\n",
    "1834    13.61\n",
    "Name: Value, Length: 100, dtype: float64\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9438d96",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "**Input 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "581e8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (100,), First value: 12.82\n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "print(f\"Shape: {weather_values.shape}, First value: {weather_values.iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef3c1b",
   "metadata": {},
   "source": [
    "**Expected outcome:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e02a97d",
   "metadata": {},
   "source": [
    "`Shape: (100,), First value: 12.82 `"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffb1692",
   "metadata": {},
   "source": [
    "⚙️ **Task:** Create a function that calculates the t-statistic and p-value. The function should accept two **single columns** of data and return a tuple of the t-statistic and p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "8e45878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "def run_ttest(Column_A, Column_B):       \n",
    "    return ttest_ind(Column_A,Column_B,alternative = 'two-sided')\n",
    "    \n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb80bb12",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b3b07adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-stat: -0.12463, p-value: 0.90083\n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "alpha = 0.05\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "print(f\"T-stat: {t_stat:.5f}, p-value: {p_val:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24a913",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "\n",
    "`T-stat: -0.11632, p-value: 0.90761`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ae8fcf",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "⚙️ **Task:** Replace the **\\<MISSING CODE>** to print out the t-test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c884eac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def print_ttest_results(station_id, measurement, p_val, alpha):\n",
    "    \"\"\"\n",
    "    Interprets and prints the results of a t-test based on the p-value.\n",
    "    \"\"\"\n",
    "    if p_val <= alpha:\n",
    "        print(f\"   Significant difference in {measurement} detected at Station  {station_id}, (P-Value: {p_val:.5f} < {alpha}). Null hypothesis rejected.\")\n",
    "    else:\n",
    "        print(f\"   No significant difference in {measurement} detected at Station  {station_id}, (P-Value: {p_val:.5f} > {alpha}). Null hypothesis not rejected.\")\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2651a66d",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "59929bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No significant difference in Temperature detected at Station  0, (P-Value: 0.90083 > 0.05). Null hypothesis not rejected.\n"
     ]
    }
   ],
   "source": [
    "# Example for station ID 0 and Temperature\n",
    "station_id = 0\n",
    "\n",
    "measurement = 'Temperature'\n",
    "\n",
    "# Filter data for the specific station and measurement\n",
    "field_values = filter_field_data(field_df, station_id, measurement)\n",
    "weather_values = filter_weather_data(weather_df, station_id, measurement)\n",
    "\n",
    "# Perform t-test\n",
    "t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "print_ttest_results(station_id, measurement, p_val, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02656fe5",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "\n",
    "`No significant difference in Temperature detected (P-Value: 0.90761 > 0.05). Null hypothesis not rejected.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353c517",
   "metadata": {},
   "source": [
    "Now we can put it all together in a loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e22bed",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "⚙️ **Task:** Create a function that loops over `measurements_to_compare` and all `station_id`, perform a t-test and print the results. The function should accept `field_df`, `weather_df`, `list_measurements_to_compare`, `alpha`. the value of `alpha` should default to a value of 0.05. Hint: use `print_ttest_results()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "89bf6f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### START FUNCTION\n",
    "\n",
    "def hypothesis_results(field_df, weather_df, list_measurements_to_compare, alpha = 0.05):\n",
    "    for station_id in [0,1,2,3,4]:\n",
    "        for measurement in list_measurements_to_compare:\n",
    "            # Filter data for the specific station and measurement\n",
    "            field_values = filter_field_data(field_df,station_id, measurement)\n",
    "            weather_values = filter_weather_data(weather_df,station_id, measurement)\n",
    "            \n",
    "            # Perform t-test\n",
    "            t_stat, p_val = run_ttest(field_values, weather_values)\n",
    "            print_ttest_results(station_id, measurement, p_val, alpha)\n",
    "\n",
    "### END FUNCTION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea120b",
   "metadata": {},
   "source": [
    "**Input:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1fafd76c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   No significant difference in Temperature detected at Station  0, (P-Value: 0.90083 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  0, (P-Value: 0.20242 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  0, (P-Value: 0.57141 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  1, (P-Value: 0.44064 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  1, (P-Value: 0.52305 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  1, (P-Value: 0.24538 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  2, (P-Value: 0.88619 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  2, (P-Value: 0.33897 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  2, (P-Value: 0.99386 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  3, (P-Value: 0.65802 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  3, (P-Value: 0.37699 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  3, (P-Value: 0.15368 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Temperature detected at Station  4, (P-Value: 0.89073 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Rainfall detected at Station  4, (P-Value: 0.33423 > 0.05). Null hypothesis not rejected.\n",
      "   No significant difference in Pollution_level detected at Station  4, (P-Value: 0.25329 > 0.05). Null hypothesis not rejected.\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "hypothesis_results(field_df, weather_df, measurements_to_compare, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f1b7b0",
   "metadata": {},
   "source": [
    "**Expected outcome:**\n",
    "```python \n",
    "   No significant difference in Temperature detected at Station 0, (P-Value: 0.90761 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 0, (P-Value: 0.21621 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 0, (P-Value: 0.56418 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 1, (P-Value: 0.47241 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 1, (P-Value: 0.54499 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 1, (P-Value: 0.24410 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 2, (P-Value: 0.88671 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 2, (P-Value: 0.36466 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 2, (P-Value: 0.99388 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 3, (P-Value: 0.66445 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 3, (P-Value: 0.39847 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 3, (P-Value: 0.15466 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Temperature detected at Station 4, (P-Value: 0.88575 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Rainfall detected at Station 4, (P-Value: 0.33237 > 0.05). Null hypothesis not rejected.\n",
    "   No significant difference in Pollution_level detected at Station 4, (P-Value: 0.21508 > 0.05). Null hypothesis not rejected.\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d75778",
   "metadata": {},
   "source": [
    "Great! There we go. For all of our measurements the p-value > alpha, so there is not enough evidence to reject the null hypothesis. This means we have no evidence to suggest that the weather data is different from the field data. This makes us confident that our field data, at least in terms of temperature, rainfall, and pollution level is reflecting the reality. \n",
    "\n",
    "Why was this important? Well, we saw from the EDA that there were some relationships, and possible correlations with the standard yield, but we really can't say what affects a crop's success, because all of them seemed to. In a sense, we as humans could not clearly see the relationships, if we were given a set of conditions like rainfall, pH, and crop type, we could not reliably estimate what the standard yield of a crop is, because the relationships are hard to understand.\n",
    "\n",
    "So our next step is to allow a machine to look for patterns, which is Machine Learning (ML). Computers are not limited to three dimensions, can calculate for hours, and find hidden patterns we cannot. Machine learning follows the basic principle across computational domains; junk in, junk out. We needed to make sure that the data we're feeding into ML models is accurate. Now we know, and we're ready for the next step. \n",
    "\n",
    "You must have been itching to get into AI, so we'll dive in soon.\n",
    "\n",
    "Until then, look after yourself!\n",
    "Saana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
