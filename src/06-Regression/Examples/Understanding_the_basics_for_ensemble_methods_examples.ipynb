{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea229f40",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Examples: Understanding the basics for ensemble methods\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this train, we'll explore ensemble methods focusing on merging multiple models for improved predictions using the Python `scikit-learn` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "- Understand different ensemble methods and apply one using the `scikit-learn` library.\n",
    "- Understand combining multiple models for enhanced predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3f0e1",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "### Understanding ensemble learning\n",
    "\n",
    "Ensemble learning is a machine learning paradigm where **multiple models** (often called \"weak learners\") **are trained to solve the same problem and combined to get better results**. The main principle behind ensemble learning is that a group of weak learners can come together to form a strong learner, thereby increasing the accuracy of predictions. Ensemble methods can be especially powerful in reducing variance, bias, or improving predictions over single-model approaches.\n",
    "\n",
    "### Types of ensemble methods\n",
    "There are several ensemble methods, each with its unique way of combining models. The most common types include:\n",
    "\n",
    "- **Bagging (bootstrap aggregating)**: It involves training multiple models in parallel, each on a random subset of the data (with replacement), and then averaging their predictions. **Random forest** is a popular example of bagging.\n",
    "\n",
    "- **Boosting**: It trains models sequentially, each trying to correct its predecessor's errors. The models are weighted based on their accuracy, and predictions are made based on the weighted sum of the predictions. Examples include **AdaBoost**, **Gradient boosting**, and **XGBoost**.\n",
    "\n",
    "- **Stacking (stacked generalisation)**: It involves training multiple models on the same data and then training a meta-model to make a final prediction based on the predictions of the previous models.\n",
    "\n",
    "- **Voting**: In voting, multiple models are trained independently, and their predictions are combined through a majority vote (in classification problems) or an average (in regression problems) to make the final prediction. This method leverages the diversity among the models to improve the overall performance.\n",
    "\n",
    "It's important to note that these are not the only ensemble methods available. There exist other, perhaps less commonly used, ensemble techniques that are not covered within the scope of this lesson. These methods further explore different strategies for model combination to improve prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661bad04",
   "metadata": {},
   "source": [
    "## Examples: Implementing an ensemble method Using `scikit-learn`\n",
    "Let's implement a basic ensemble method using the `scikit-learn` library. We'll use the dataset provided to predict the `BiodiversityHealthIndex` using both a single model and an ensemble method for comparison. For the ensemble, we'll use a `RandomForestRegressor`, a popular bagging method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dba616",
   "metadata": {},
   "source": [
    "### Step 1: Preparing the data\n",
    "We'll start by preparing our data for modelling. This involves splitting the data into features (`X`) and the target (`y`), and then splitting these into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62341cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/SDG_15_Life_on_Land_Dataset.csv')\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop('BiodiversityHealthIndex', axis=1)\n",
    "y = data['BiodiversityHealthIndex']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d892fee",
   "metadata": {},
   "source": [
    "### Step 2: Building individual models\n",
    "\n",
    "Let's first build a simple decision tree regressor as our weak learner for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6477dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree MSE: 0.15674493632579156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Initialise and train the decision tree\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "tree_predictions = tree_model.predict(X_test)\n",
    "tree_mse = mean_squared_error(y_test, tree_predictions)\n",
    "print(f\"Decision Tree MSE: {tree_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9956efe",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 3: Building an ensemble model\n",
    "\n",
    "Now, let's use the `RandomForestRegressor` as our ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "466cb7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest MSE: 0.08985732245806438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialise and train the random forest\n",
    "forest_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "forest_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "forest_predictions = forest_model.predict(X_test)\n",
    "forest_mse = mean_squared_error(y_test, forest_predictions)\n",
    "print(f\"Random Forest MSE: {forest_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c543f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "By comparing the Mean Squared Error (MSE) of the decision tree model with that of the random forest, we can observe the impact of using ensemble methods. Typically, the random forest (an ensemble method) should outperform the single decision tree due to its ability to reduce overfitting and variance in predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
