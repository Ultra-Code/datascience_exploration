{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ee-v8I_P8rfp"
   },
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Exercise.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>\n",
    "\n",
    "# Exercise: KNN and Naive Bayes\n",
    "© ExploreAI Academy\n",
    "\n",
    "In this exercise, we train a k-nearest neighbours model and experiment with various parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITqrvAqq8xSC"
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "* Train a k-nearest neighbours model.\n",
    "* Compare KNN models trained with different parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "K-nearest neighbours (KNN) is a simple, instance-based learning algorithm in which an observation's classification is determined by the majority vote of its neighbours. In this exercise, we will explore the KNN model's sensitivity to the choice of the number of neighbours (K) and the type of distance metric used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used in this exercise is the `Breast Cancer` dataset provided by `scikit-learn`. This dataset comprises features crucial for distinguishing between malignant (cancerous) and benign (non-cancerous) tumours.\n",
    "\n",
    "We aim to apply KNN models to this data to accurately classify malignant and benign tumours based on cell characteristics to aid in early diagnosis, guide treatment decisions, and potentially improve patient outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then split the data to prepare the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Let's experiment with different values of k to understand how it affects the accuracy and generalisation of the model.\n",
    "\n",
    "Use a for loop to train different k-nearest neighbours models, each with a different number of neighbours as follows: `1, 3, 5, 7, 9`.\n",
    "Evaluate each model's performance on the test set and print its accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Let's also compare the impact that different distance metrics will have on the accuracy of the KNN model.\n",
    "\n",
    "Again, use a for loop to train different k-nearest neighbours models, each with a different distance metric as follows: `Euclidean, Manhattan, Chebyshev`. Evaluate each model's performance on the test set and print its accuracy score.\n",
    "\n",
    "**Note:** You can use what seems to be the optimal number of neighbours based on the results from Exercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "**a)** \n",
    "\n",
    "We want to be able to train a KNN model using a given k and a distance metric. We will then plot the decision boundary to better understand how the model classifies different regions of the feature space based on the chosen k and metric.\n",
    "\n",
    "The function below contains the code for visualising the decision boundary. You are required to complete it by adding appropriate parameters (this should include the feature dataset, the target dataset, the number of neighbours, and the distance metric) and lines of code such that it performs the function stated above.\n",
    "\n",
    "**Note:** We train the model using only two features for ease of plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def plot_decision_boundary(# Add parameters):\n",
    "    # Add your code here\n",
    "\n",
    "    # Create a mesh grid based on feature ranges\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Predict class for each point on the grid\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contours\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')  # Plot the training points\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f\"Decision Boundary for K={k} with {metric} metric\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)** \n",
    "\n",
    "Use the function created in **section a** to plot the decision boundary of a KNN model trained using `5` nearest neighbours and the `Euclidean` distance metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of different values of k to evaluate\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "\n",
    "# List to store accuracy scores of each KNN model for comparison\n",
    "scores = []\n",
    "\n",
    "# Loop over each value of k \n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    scores.append(accuracy)\n",
    "    print(f\"Accuracy for K={k}: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the for loop, a new instance of `KNeighborsClassifier` is created with the current `k` from the list. This classifier will consider \n",
    "`k` nearest neighbours to make predictions. The accuracy score for each `k` is then appended to the `scores` list and later printed for comparison.\n",
    "\n",
    "Typically, a smaller k results in a model that captures more noise and details of the training data, potentially leading to overfitting. On the other hand, a larger k tends to smooth out the decision boundaries, which can improve generalisation but may also lead to underfitting if k is too large. \n",
    "\n",
    "From the results for our different k-values, we see that the accuracy stops increasing past `k=5`. This suggests that `k=5` could be the optimal choice for good accuracy while minimising the risk of overfitting or underfitting.\n",
"\n",
"**Experiment**: What happens if you expand this list of k-values up to 20? Does this change your mind on which value might be the optimal one?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of distance metrics to evaluate\n",
    "metrics = ['Euclidean', 'Manhattan', 'Chebyshev']\n",
    "\n",
    "# Loop over distance metric\n",
    "for metric in metrics:\n",
    "    knn = KNeighborsClassifier(n_neighbors=5, metric=metric)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy with {metric} distance: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loop iterates over each metric specified in the `metrics` list. For each metric, a new instance of `KNeighborsClassifier` is created with `5` neighbours and the specified metric. The accuracy for each metric is then printed out.\n",
    "\n",
    "We observe that although the differences may be subtle, the choice of distance metric can influence the accuracy of a KNN model depending on the dataset and the features' characteristics. \n",
    "\n",
    "From our results, it seems that the `Euclidean` and `Chebyshev` distances are equally effective for this particular dataset, while `Manhattan` performs slightly differently – which could be due to the nature of the dataset, such as scale and distribution of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "**a)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot decision boundaries \n",
    "def plot_decision_boundary(X, y, k, metric):\n",
    "    # Train a KNN model using the specified number of neighbours and distance metric\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "    knn.fit(X[:, :2], y)  # Use only two features for simplicity\n",
    "\n",
    "    # Create a mesh grid based on feature ranges\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "\n",
    "    # Predict class for each point on the grid\n",
    "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot the contours\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')  # Plot the training points\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title(f\"Decision Boundary for K={k} with {metric} metric\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the following parameters to the function: \n",
    "- `X`: The feature dataset.\n",
    "- `y`: The target dataset.\n",
    "- `k`: The number of neighbours to use for the KNN classifier.\n",
    "- `metric`: The distance metric to use in the KNN classifier.\n",
    "\n",
    "We then add code to set up the KNN classifier with the specified number of neighbours and distance metric and train it using only the first two features of the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting decision boundary\n",
    "plot_decision_boundary(X_train, y_train, 5, 'Euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call the `plot_decision_boundary` function and pass the appropriate arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By experimenting with various values of k and different distance metrics, we have gained insight into how the parameters of the KNN model affect its performance. \n",
    "\n",
    "\n",
    "In our case, selecting an optimal value of k, around 5, and using the Euclidean distance metric provided the best balance between accuracy and model complexity for this particular dataset. We can use this as a basis to fine-tune our model further through other model-improvement techniques. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZFCZhaikX+N2/Bg7W6SY+",
   "collapsed_sections": [],
   "name": "Search_algorithms.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "6b5ebbc2c6bde2831bc6c0426f75aca8137ccfc69d329557556ed73faee126ae"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
