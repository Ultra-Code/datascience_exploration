{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f662d169",
   "metadata": {},
   "source": [
    "# Examples: Neural network classifiers\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af890c",
   "metadata": {},
   "source": [
    "In this train, we'll get an overview of Neural Networks as advanced classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d230d14",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "- Obtain a basic understanding of the architecture and fundamental elements of an artificial neural network.\n",
    "- Understand how to use TensorFlow layers to build a neural network architecture.\n",
    "- Understand how a model is trained and evaluated using a validation split.\n",
    "- Implement an effective neural network for classification using keras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97eb2f08",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "An artificial neural network is a highly parameterised but non-parametric model used for machine learning. We will henceforth refer to it as a neural net or an NN. A NN model can be fit to a set of inputs (X) and outputs (y), like the other regression and classification algorithms we have covered so far. Where a NN differs is in its generalisabilty and the ability to implement non-linear architecture with relative ease. Here, generalisability refers to the ability of a model to adapt properly to new, unseen data drawn from the same distribution as the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6519698",
   "metadata": {},
   "source": [
    "## Fundamental components\n",
    "\n",
    "Neural nets are highly complex structures which we could spend any arbitrary length of time discussing. For the purpose of brevity, we will narrow our focus to four main areas: layers, the neuron, weights, and activation functions. \n",
    "\n",
    "### Layers\n",
    "\n",
    "Taking the figure below as a reference, we can see that the first segmentation we can make of a single neural net model is into layers. Data is fed in at the left hand side, into the single input layer. It is then passed to and transformed by some arbitrary (up to the designer) number of what are called *hidden layers*, making up the meat of the internals of the model. The last layer, at the far right, is known as the output layer - this is where we collect the predictions we were hoping for. In classification, output values are typically designed to be probabilities of belonging to each of the classes.  \n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-nn-structure.png\" alt=\"sketch-nn-structure\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8b0d2",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "The fundamental unit of which each layer is comprised is known as the neuron. The neuron is effectively a rendezvous point for some number of inputs, where they are all added together and sometimes modified, before being sent on to the next neuron in the network.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-neuron-diagram.png\" alt=\"sketch-neuron-diagram\" width=\"500px\">\n",
    "\n",
    "As portrayed in the figure above, some number of inputs arrive at the neuron. In the input layer, the input to a neuron is simply a feature value (like someone's age, income, or a pixel value in an image). For neurons in the hidden or output layers, the input to a neuron is the product of the output of another neuron (indicated by $x_1, x_2, ..., x_p$) and a corresponding weight (indicated by $w_1, w_2, ..., w_p$), which we will discuss below.\n",
    "\n",
    "Neurons can be connected to any number of neurons from the preceding layer, and are usually connected to all of them. Hidden layers whose neurons are each connected to all of the neurons in the preceding layer are called _dense_ or _fully connected_ layers.\n",
    "\n",
    "Essentially, the only job of the neuron is to sum all of the inputs it receives, add some bias term $\\beta$, and ensure that that sum is passed through an activation function. Neurons are also known as _nodes_ or _units_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4d19b",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "Once all of the inputs arriving at the neuron have been summed, they are passed through an activation function. Activation functions are not set on a per-neuron basis, but rather on a per-layer basis. As we will see below, when we manually add each layer to the network, we need to specify the activation function that goes along with it.\n",
    "\n",
    "We might consider the role of the activation function to be like a switch, which declares whether or not the output from the neuron in question is activated (\"fired\"). This firing is dependent on the value coming out of the neuron. Some activation functions take that one step further, and change the magnitude of the output, thereby changing the affect that that neuron's output has on the whole network.\n",
    "\n",
    "An activation function is simply a mathematical function which takes as its input the output of the neuron. It helps to introduce non-linearity into our model, and for the most part, they normalise the output of the neuron to be between 0 and 1. Two common activation functions are the ReLU (Rectified Linear Unit), and the Softmax. They each have special properties that allow our model to learn non-linear relationships in our data, and are defined as follows:\n",
    "\n",
    "$$\n",
    "ReLU(x) = \n",
    "\\begin{cases}\n",
    "0 &\\mbox{if } x < 0 \\\\\n",
    "x &\\mbox{if } x \\geq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Softmax(x)_i = \\frac{e^{x_i}}{\\sum_i e^{x_i}}\n",
    "$$\n",
    "\n",
    "The ReLU function basically just \"turns off\" a neuron if its input is less than 0, and is linear if its input is greater than zero. The Softmax function is a little different - it applies the exponential function to each of its units, and makes sure the layer as a whole is normalized (i.e., sums to one). We typically use it within the output layer of a NN when we want a categorical probability distribution (such as when predicting which class given input data belongs to). In the figure below, check out the distribution that the Softmax activation function might produce for an input image containing the number 2, from a layer with 10 neurons.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/sketch-softmax.png\" alt=\"sketch-softmax\" style=\"width: 450px;\"/>\n",
    "\n",
    "Other activation functions, such as **sigmoid** and **tanh** do exist, but for now we'll only focus on ReLU and Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d896e8a",
   "metadata": {},
   "source": [
    "### Weights\n",
    "\n",
    "For every output from a neuron in the network, there is a weight. A weight is a model parameter; in other words, one of the values which changes as the model learns during the training process - but not one that we have access to changing.\n",
    "\n",
    "Each weight is a single value, and the corresponding output is multiplied by the weight before it reaches the next neuron. Once the weight and the neuron output have been multiplied together, we can consider that product to be an input to the next neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41150a1d",
   "metadata": {},
   "source": [
    "### A few more definitions\n",
    "\n",
    "Before we carry on, it will be helpful to expound on a couple more terms that will be used during the modelling process.\n",
    "\n",
    "- **Deep learning** is simply a subset of machine learning in which we specifically use 'deeper' neural network architectures to perform our modeling. The depth of a neural net refers to the increasing number of hidden layers it possesses.\n",
    "\n",
    "- An **epoch** refers to the number of times the entire dataset is passed through the network. If we set `epochs=1`, the model will only see each individual observation exactly once.\n",
    "\n",
    "- **Batch size** refers to the number of observations in a group, known as a batch, which are sent through the network during training before the parameters (weights) are updated. An epoch might require many batches to complete. For example, if we have 1000 observations and a batch size of 100, one epoch will require 10 batches to complete.\n",
    "\n",
    "- To **flatten** means to convert to a vector of a single dimension. For example, a matrix of dimensions $(3, 16)$ after flattening would have dimensions $(48,)$.\n",
    "\n",
    "- The term **feedforward** is used to describe a network where the neurons in each layer are only connected to neurons in subsequent layers (and not any preceding layers). Networks in which neurons are connected to neurons from preceding layers, or indeed to themselves, are known as recurrent networks.\n",
    "\n",
    "- **TensorFlow** is an open-source machine learning library which is used to carry out deep learning tasks. **Keras** is a high-level library provided within TensorFlow. It is possible to write TensorFlow code directly, but Keras makes it much easier and intuitive through APIs and logical syntax. We will use Keras to build models in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595de771",
   "metadata": {},
   "source": [
    "## Architecture of a neural network\n",
    "\n",
    "Using what we have learned above about neural nets, let's try and put it all together and solve a problem.\n",
    "\n",
    "Let's say we want to create a network that will recognise handwritten digits in images. It should take as input a 28x28 image (a total of 784 features) and output a 10x1 vector, where the digit predicted to be in the image is indicated by a value of 1 in the position in the vector of that digit. For example, if we input an image of a 2, we would expect an output vector of [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] - the 3rd position, indicating the '2' digit, has a 1 in it.\n",
    "\n",
    "We want to add two hidden layers in order to introduce some non-linearity into our model. The architecture for this model looks as follows:\n",
    "\n",
    "<img src=\"https://github.com/Explore-AI/Public-Data/blob/master/Neural%20Networks/ANN_arc.png?raw=true\" width=\"600px\">\n",
    "\n",
    "Let's break this down:\n",
    "\n",
    "- The input image is 28x28 pixels, for a total of 784 pixels. Hence the input layer has 784 neurons: one for each pixel value.\n",
    "\n",
    "- The network has two hidden layers, each with 64 neurons, and each followed by ReLU-type activation functions.\n",
    "\n",
    "- It has an output layer containing 10 neurons, followed by a softmax activation. This is where we will get the 10x1 vector.\n",
    "\n",
    "- Finally, a loss is calculated from the output layer (this is usually not included in the final product)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07daa4b",
   "metadata": {},
   "source": [
    "## The MNIST dataset\n",
    "\n",
    "The MNIST dataset is a classic problem set in machine learning, containing 60,000 28Ã28 pixel grayscale images of single, handwritten digits from 0 to 9. The task is to classify a given image of a handwritten digit into one of 10 classes.\n",
    "\n",
    "It is a widely used and deeply understood dataset and, for the most part, is âsolved.â Top-performing models are deep learning convolutional neural networks that achieve a classification accuracy of above 99%, with an error rate between 0.4 %and 0.2% on the hold-out test dataset. However for the scope of this train we will intentionally be avoiding CNNs for now.  \n",
    "\n",
    "In the code below we'll learn how to implement a NN using **Tensorflow** and how how to train it to recognise handwritten images.\n",
    "\n",
    "**Note:** The code below will run significantly faster if you have access to a GPU. One approach might be to upload this train to [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true) and setting the runtime type to \"GPU\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8205873",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Before we can build any neural networks we need to import a few things from Tensorflow.\n",
    "\n",
    "**Note:** the following installation is quite large ~550MB and may take some time to download and install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f635ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28248b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from matplotlib import pyplot as plt\n",
    "from random import randint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39aa1f0",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "Next we load our dataset (MNIST, using `Keras'` dataset utilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08915f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the dataset\n",
    "# Setup train and test splits\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Making a copy before flattening for the next code-segment which displays images\n",
    "x_train_drawing = x_train\n",
    "\n",
    "print(x_train.shape) # (60000, 28, 28)\n",
    "print(y_train.shape) # (60000,) \n",
    "\n",
    "# Convert class vectors to binary class matrices\n",
    "num_classes = 10\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6672ce1",
   "metadata": {},
   "source": [
    "We need to flatten each image before we can pass it into our neural network. Weâll also normalize the pixel values from [0, 255] to [-0.5, 0.5] to make our network easier to train (using smaller, centered values is often better)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdc55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the images.\n",
    "x_train = (x_train / 255) - 0.5\n",
    "x_test = (x_test / 255) - 0.5\n",
    "\n",
    "# Flatten the images.\n",
    "x_train = x_train.reshape((-1, 784))\n",
    "x_test = x_test.reshape((-1, 784))\n",
    "\n",
    "print(x_train.shape) # (60000, 784)\n",
    "print(x_test.shape)  # (10000, 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd11d88",
   "metadata": {},
   "source": [
    "## A look at some random digits\n",
    "\n",
    "It's a good idea to get a sense of the dataset we're working with. Run this code multple times to see new randomly selected digits from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89388a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(64):\n",
    "    ax = plt.subplot(8, 8, i+1)\n",
    "    ax.axis('off')\n",
    "    plt.imshow(x_train_drawing[randint(0, x_train.shape[0])], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89931c",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "\n",
    "Every Keras model is either built using the `Sequential` class, which represents a linear stack of layers, or the functional `Model` class, which is more customisable. Weâll be using the simpler Sequential model, since our network is indeed a linear stack of layers.\n",
    "\n",
    "We start by instantiating a Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bc138",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  # layers...\n",
    "    \n",
    "]),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f0118e",
   "metadata": {},
   "source": [
    "The **Sequential constructor** takes an array of Keras Layers. Since weâre building a standard feedforward network, we only need the `Dense` layer, which is the regular fully-connected (dense) type.\n",
    "\n",
    "Letâs add three dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e6dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(64, activation='relu'),\n",
    "  Dense(10, activation='softmax'),\n",
    "]),"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cabc2c",
   "metadata": {},
   "source": [
    "The first two layers have 64 neurons each and use the ReLU activation function. The last layer is a has 10 neurons, one for each class, and uses the Softmax activation function. Remember, neural network classifiers commonly use Softmax as the activation function for the output layer, as it produces an easily interpretable output. Scroll back up to the Softmax graph figure to see why.\n",
    "\n",
    "The last thing we need to do is tell `Keras` what our networkâs input will look like. We can do that by specifying an `input_shape` to the first layer in the sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3254b4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(784,)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd5d70f",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "Before we can begin training, we need to configure the training process. We decide on three key factors during the compilation step.\n",
    "\n",
    "1. The **optimizer**: an algorithm which directly controls the rate and method by which weights are changed during training. `Keras` has a number of optimizer options; we'll stick with `Adam`, a common gradient-based method.\n",
    "2. The **loss function**. Since weâre using a Softmax output layer, weâll use the Cross-Entropy loss. Keras distinguishes between `binary_crossentropy` (two classes) and categorical_crossentropy (>2 classes), so weâll use the latter as we're trying to classify 10 different digits.\n",
    "3. A list of **metrics**. Since this is a classification problem, weâll just have `Keras` report on the accuracy metric. Remember, for a dataset with imbalanced classes, something like _F1-score_ is a better choice.\n",
    "\n",
    "Hereâs what that compilation looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a69054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb5d958",
   "metadata": {},
   "source": [
    "## Training and evaluating the model\n",
    "\n",
    "This code trains and evaluates the model we have created. It also uses `matplotlib` and the `history` object provided by Keras, to report on how the model behaves during training. Here, we are using the `history` object it to plot training and validation accuracy over time.\n",
    "\n",
    "Training the model is as simple as calling the `.fit()` method and specifying some hyperparameters. There are a lot of possible hyperparameters, but weâll only manually supply a few:\n",
    "\n",
    "- The training data (images and labels), commonly known as X and Y, respectively;\n",
    "- The number of epochs (described above);\n",
    "- The batch size (described above); and\n",
    "- The validation split (described below).\n",
    "\n",
    "Hereâs what that looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3060ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3c3d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=10, verbose=True, validation_split=0.1)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show()\n",
    "\n",
    "print(f'Test loss: {loss:.3}')\n",
    "print(f'Test accuracy: {accuracy:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe823c60",
   "metadata": {},
   "source": [
    "Not bad for a relatively simple model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25390ebe",
   "metadata": {},
   "source": [
    "## What is the role of the `validation_split` argument?\n",
    "\n",
    "In the `.fit()` method we specified `validation_split=0.1`. What this does is as follows. During each epoch:\n",
    "\n",
    "1. The bottom 0.1, or 10%, of the observations specified in `x` and `y`, are held back and not used for training.\n",
    "2. Once training has completed for that epoch, the 10% of rows which were held back are used to validate the model, providing the results for `val_acc` that we see after each epoch.\n",
    "\n",
    "Using a validation subset gives us insight to guide our model training. Unlike other machine learning models we've studied which are able to appropriately terminate learning after reaching some internal stopping criterion, training a neural network is a *non-convex optimisation task* - which in simple terms means that it is difficult to know if we have an adequate model by just looking at training set performance. This is further complicated by the high capacity nature of neural networks; with often hundreds of thousands of parameters available, these models tend to easily overfit (and in some cases memorize) the data they are trained on. \n",
    "\n",
    "To make this point practical, by taking a look at the training and validation accuracy versus epoch curves we can see how validation tends to slowly increase and then flatten out. If at any stage we can see the validation curve beginning to drop consistently, it is a sure sign of overfitting and a good place for training to cease. In actual fact, this observation is an important  part of a popular regularization technique know as *Early Stopping*, where we set the number of learning epochs to be a large value, and expect to terminate training when validation performance plateaus. More info about this [here](https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad57461",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa42fef2",
   "metadata": {},
   "source": [
    "We now want to use our model to make a prediction on a given image. Can our model accurately determine the handwritten number depicted?\n",
    "\n",
    "Run the code below a few times. It will show the image we are trying to predict, as well as the probability distribution that the model outputs. You'll start to get a sense of what the model is seeing as you cycle through some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f4ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select an image at random to feed to our model\n",
    "idx = np.random.randint(len(x_train))\n",
    "\n",
    "prediction = model.predict(x_train[[idx]])[0]\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].imshow(x_train_drawing[idx], cmap='Greys')\n",
    "ax[0].set_xticks(())\n",
    "ax[0].set_yticks(())\n",
    "ax[0].set_title('Image fed to model')\n",
    "\n",
    "ax[1].bar(range(0, 10), prediction)\n",
    "ax[1].set_xticks(range(0, 10))\n",
    "ax[1].set_xlabel('Number')\n",
    "ax[1].set_ylabel('Probability')\n",
    "ax[1].set_title('Output probability distribution')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(f'Top prediction: {prediction.argmax()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb27ff54",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train we have seen or been introduced to:\n",
    "\n",
    "- The fundamental ingredients of a neural network, including layers, neurons, weights, activation functions;\n",
    "- The famous MNIST dataset for image recognition tasks;\n",
    "- The construction of a neural network for image classification using `keras`;\n",
    "- The virtue of keeping a validation set to guide model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd094b-0fee-46f1-a4b8-73766813c42b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
