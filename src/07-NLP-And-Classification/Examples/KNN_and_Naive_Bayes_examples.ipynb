{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples: KNN and Naive Bayes\n",
    " \n",
    "© ExploreAI Academy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this train, we look at the theory underlying the k-nearest neighbours and Naive Bayes models and how to implement and evaluate them in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning objectives\n",
    "\n",
    "By the end of this train, you should be able to:\n",
    "\n",
    "* Understand how to train and implement Naive Bayes classifiers.\n",
    "* Understand how the log loss function works and how it differs from other classification metrics.\n",
    "* Understand how to train and implement k-nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Naive Bayes theory\n",
    "\n",
    "> **Note:** The following section is conceptually deep and is provided for comprehensiveness. The theory will not be examined in your tests. If you feel uncomfortable at any point in following along with the explanations provided, you're welcome to jump over to the coding sections. You can always return later when you feel like you're ready for a challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a classification algorithm that uses the principle of [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) to make classifications. \n",
    "\n",
    "The benefits of Naive Bayes are that the model is **simple** to build and is useful on **large datasets**. \n",
    "\n",
    "Further, the model makes an explicit **assumption** that the **features are independent** given the class label. What does this mean? Well first let's consider the concept of independence. Independence is a concept from probability theory and it implies that if we have two random variables $X$ and $Y$, then\n",
    "\n",
    "$$\n",
    "P(X \\cap Y) = P(X)P(Y)\n",
    "$$\n",
    "\n",
    "This is where the qualifier \"Naive\" in \"Naive Bayes\" comes from. The assumption is *Naive* because it often does not hold. The assumption of independence implies that the model assumes that there is **zero correlation** among the features. Hence, the joint probability distribution $P(X, Y)$ can be obtained from the marginal probability distributions $P(X)$ and $P(Y)$ simply by multiplication. We will use the above independence assumption, conditional probability rules, and Bayes' theorem to develop some theory for how the Naive Bayes model works.\n",
    "\n",
    "Next, let's consider Bayes' theorem. Bayes' theorem is an important result in statistics and it allows us to obtain a posterior distribution given a prior distribution and a likelihood. Now that is a mouthful, but don't worry, we will walk you through it. First, let's denote class labels using $K = 1, \\cdots, k$ – seem familiar? What we are trying to say here is that each class label corresponds to a number between $1$ and $k$. So, given this, the independence assumption conditional on the class label is denoted:\n",
    "\n",
    "$$\n",
    "P(X_{1} \\cap X_{2} \\mid K = k) = P(X_{1} \\mid K = k)P(X_{2} \\mid K = k)\n",
    "$$\n",
    "\n",
    "In the expression above, $X_{1}$ and $X_{2}$ denote random predictor variables. Keep this in mind. Bayes' theorem states that\n",
    "\n",
    "$$\n",
    "P(K = k \\mid X) = \\frac{P(X \\mid K= k) P(K = k)}{P(X)}\n",
    "$$\n",
    "\n",
    "In the expression above:\n",
    "\n",
    "- $P(K = k \\mid X)$ is called the _posterior probability_ distribution.\n",
    "\n",
    "- $P(K = k)$ is the *prior*.\n",
    "\n",
    "- $P(X \\mid K= k)$ is the _likelihood_.\n",
    "\n",
    "The posterior tells us what the probability is of the class being $k$ given a particular observation. The prior is basically a measure of how likely we think it is for any observation to be assigned to a particular class before we have observed any observations. The likelihood gives us a measure of what the data says about the probability that the observation belongs to class $k$.\n",
    "\n",
    "That's it. In principle, this is how the Naive Bayes model works (i.e. with two predictor variables and $k$ classes). Here's the general algorithm:\n",
    "\n",
    "1. For each class $k$:\n",
    "\n",
    "    - Find the likelihood $P(X_{1} \\cap X_{2} \\mid K = k) = P(X_{1} \\mid K = k)P(X_{2} \\mid K = k)$, using only the observations where the class is $k$ in the data. \n",
    "    - Compute a prior probability for the current class $k$ = $\\frac{observations \\space in \\space class \\space k}{total \\space number \\space of \\space observations}$. \n",
    "    - Use Bayes' theorem, with a denominator (i.e. the evidence $P(X)$) of $1$, to compute the posterior probability distribution.\n",
    "    \n",
    "    \n",
    "2. At test time, observations are assigned to classes with the highest posterior probability $P(X_{1} \\cap X_{2} \\mid K = k)$.\n",
    "\n",
    "\n",
    "Now that we've covered the intuition, let's generalise the model to include an arbitrary number of random predictor variables instead of 2, i.e.\n",
    "\n",
    "$$\n",
    "X_{1}, \\cdots, X_{p}\n",
    "$$\n",
    "\n",
    "In this case:\n",
    "\n",
    "$$\n",
    "P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) = \\frac{P(X_{1} \\cap X_{2} \\cdots \\cap X_{p} \\mid K= k) P(K = k)}{P(X)}\n",
    "$$\n",
    "\n",
    "Now remember the assumption in the back of your mind. This allows us to simplify the expression above to become:\n",
    "\n",
    "$$\n",
    "P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) = \\frac{P(X_{1}\\mid K= k) P(X_{2}\\mid K= k) \\cdots P(X_{p}\\mid K= k) P(K = k)}{P(X)}\n",
    "$$\n",
    "\n",
    "A special mathematical symbol allows us to represent the product $P(X_{1}\\mid K= k) P(X_{2}\\mid K= k) \\cdots P(X_{p}\\mid K= k)$ as $\\prod_{i = 1}^{p} P(X_{i}\\mid K= k)$. Hence, the expression above becomes:\n",
    "\n",
    "\n",
    "$$\n",
    "P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) = \\frac{\\prod_{i = 1}^{p} P(X_{i}\\mid K= k)P(K = k)}{P(X)}\n",
    "$$\n",
    "\n",
    "Now let's consider how the Bayes classifier assigns observations to a particular class. One method, called [maximum a posteriori (MAP)](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation), maximises $P(K = k \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p})$ for $K = 1, \\cdots, k$ to assign the observation to the correct class. Let's consider what this means for $K = 2$. In this case we calculate:\n",
    "\n",
    "$$\n",
    "P(K = 1 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) \\\\\n",
    "P(K = 2 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) \\\\\n",
    "$$\n",
    "\n",
    "If $P(K = 1 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p}) > P(K = 2 \\mid X_{1} \\cap X_{2} \\cdots \\cap X_{p})$ then we say that the observation $\\underline{X}$ is assigned to class $1$ else it is assigned to class $2$. This concept can be generalised to more classes. We just want to make something clear here.\n",
    "\n",
    "\n",
    "An observation, i.e. something that actually happened, is denoted:\n",
    "\n",
    "$$\n",
    "\\underline{x}_{i} = (x_{i,1}, x_{i,2}, \\cdots, x_{i,p})\n",
    "$$\n",
    "\n",
    "But a set of random variables, which captures all possible things that can happen for all observations, is denoted:\n",
    "\n",
    "$$\n",
    "\\underline{X} = (X_{1}, X_{2}, \\cdots, X_{p})\n",
    "$$\n",
    "\n",
    "Using this notation, $X_{i}, i = 1 \\cdots, p$ denotes a feature. Let's get to some code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Naive Bayes in Scikit learn\n",
    "\n",
    "`sklearn` provides three implementations of the Naive Bayes method:\n",
    "\n",
    "   a) **Gaussian:** It is used in classification and it assumes that features follow a normal distribution.\n",
    "\n",
    "   b) **Multinomial:** It is used for discrete counts. For example, let’s say  we have a text classification problem. Here we can consider Bernoulli trials which is one step further, and instead of “word occurring in the document”, we have “count how often word occurs in the document”. You can think of it as “number of times outcome number x_i is observed over the n trials”.\n",
    "\n",
    "   c) **Bernoulli:** The binomial model is useful if your feature vectors are binary (i.e. zeros and ones). One application would be text classification with a ‘bag of words’ model where the 1s and 0s are “word occurs in the document” and “word does not occur in the document” respectively.\n",
    "\n",
    "These implementations are merely the choice of our probability distribution $P$. We choose an implementation based on the nature of the features (i.e. predictor variables) in our data.\n",
    "\n",
    "Source: https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started\n",
    "\n",
    "In this train, we will consider the **Gaussian Naive Bayes model**. You are encouraged to try the other models out as well. \n",
    "\n",
    "First we import the libraries that we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, log_loss\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make use of the Breast Cancer Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the breast cancer data\n",
    "X, y = load_breast_cancer(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and testing data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Define the model \n",
    "naive_bayes = GaussianNB()\n",
    "# Fit the model \n",
    "naive_bayes.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess model performance\n",
    "\n",
    "So far, we've been using the classification report and confusion matrices to assess classification model performance. However, such metrics don't do a good job at highlighting how confident our model is in its predictions. \n",
    "\n",
    "Enter the **log loss function** which, unlike other metrics, can penalise predictions based on how confident a model is with those predictions. For example, if our model predicts the wrong class with high probability, the log loss penalises it more (i.e.: assigns higher log loss) compared to a model that predicts the wrong class with low probability. \n",
    "\n",
    "As such, we generally feed **class probabilities** into the log loss function instead of the actual class predictions (i.e. thresholded probabilities). \n",
    "\n",
    "For the log loss metric, **lower is better**, i.e. a perfect model would have a log loss of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set \n",
    "y_hat = naive_bayes.predict_proba(X_test)\n",
    "# Calculate the log loss (this was imported from sklearn above somewhere) \n",
    "print(\"The log loss error for our model is: \", log_loss(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the decision boundary \n",
    "\n",
    "It is recommended to visualise the decision boundary of our classifier where possible. Unfortunately, in this case, we have around 31 different variables (features + response) and we can visualise only three at a time. So let's do just that. \n",
    "\n",
    "Below, we create a visualisation which can represent the decision boundary between any two features in $X$. The $y$ (i.e. class label) is indicated by the colour of each data point. The decision boundary is the line separating the two regions of blue and red, such that any point falling into the red region is assigned the red label (class 1), and any point falling into the blue region is assigned the blue label (class 2).\n",
    "\n",
    "To change which features you want to compare, simply change the values for `i` and `j`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0; j = 1\n",
    "naive_bayes.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    " \n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = naive_bayes.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature '+str(i))\n",
    "ax1.set_ylabel('Feature '+str(j))\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. K-nearest neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 K-nearest neighbours theory\n",
    "\n",
    "K-nearest neighbours (KNN) is a powerful yet easy-to-understand machine learning algorithm. In principle, this algorithm works by assigning the majority class of the N closest neighbours to the current data point. As such, absolutely no training is required for the algorithm! All we do is choose k (i.e. the number of neighbours to consider), choose a distance function to calculate proximity, and we're good to go. \n",
    "\n",
    "A typical KNN algorithm works as follows:\n",
    "\n",
    "1. Choose k (number of neighbours).\n",
    "2. Choose distance metric, e.g. Euclidean distance: \n",
    "$$\n",
    "d(X^a,X^b) = \\sqrt{(x^{a}_{1}-x^{b}_{1})^2 + (x^{a}_{2}-x^{b}_{2})^2 + \\cdots + (x^{a}_{n}-x^{b}_{n})^2}\n",
    "$$ \n",
    "3. For each data point $X_{test}$ in the testing data:\n",
    "    - For each data point $X_{train}$ in the training data:\n",
    "        - Calculate the distance $d(X_{train},X_{test})$ between the test point and training observation.\n",
    "    - Find labels of the k closest data points to $X_{test}$\n",
    "    - Assign most-frequent (i.e. the mode) class label to $X_{test}$.\n",
    "\n",
    "In the context of regression, we would use the mean of the K-nearest neighbours instead of the mode of the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 K-nearest neighbours in Scikit learn\n",
    "\n",
    "#### Fit the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3 # <--- change this number to play around with how many nearest neighbours to look for.\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors)\n",
    "# Fit the model \n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess model performance\n",
    "\n",
    "Like before, let's have a look at the **log loss**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on the test set \n",
    "y_hat = knn.predict_proba(X_test)\n",
    "# Calculate the loss \n",
    "print(\"The log loss error for our model is: \", log_loss(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the log loss is higher than it was for the Naive Bayes model. _Why do you think this is the case?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the decision boundary\n",
    "\n",
    "We once again visualise the decision boundary of our KNN classifier. Remember to change `i` and `j` to compare different features to one another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0; j = 2\n",
    "knn.fit(X[:, [i, j]], y)\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "x_min, x_max = X[:, i].min(), X[:, i].max()\n",
    "y_min, y_max = X[:, j].min(), X[:, j].max()\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))\n",
    "\n",
    "y_hat = knn.predict(np.concatenate((xx.reshape(-1,1), yy.reshape(-1,1)), axis=1))\n",
    "y_hat = y_hat.reshape(xx.shape)\n",
    "\n",
    "ax1.pcolormesh(xx, yy, y_hat, cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.scatter(X[:, i], X[:, j], c=y, edgecolors='k', cmap=plt.cm.get_cmap('RdBu_r'))\n",
    "ax1.set_xlabel('Feature '+str(i))\n",
    "ax1.set_ylabel('Feature '+str(j))\n",
    "ax1.set_xlim(xx.min(), xx.max())\n",
    "ax1.set_ylim(yy.min(), yy.max())\n",
    "ax1.set_xticks(())\n",
    "ax1.set_yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this train, we have seen or been formally introduced to:\n",
    "- Naive Bayes classifiers and underlying theory.\n",
    "- The log loss function.\n",
    "- The k-nearest neighbours model.\n",
    "- Visualising decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
