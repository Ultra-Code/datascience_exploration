{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/Python-Notebook-Banners/Examples.png\"  style=\"display: block; margin-left: auto; margin-right: auto;\";/>\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHt75rqnYDo5"
   },
   "source": [
    "# Examples: Build all the classifiers!\n",
    "Â© ExploreAI Academy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore building multiple types of classification models for the MBTI dataset and evaluate their performance using cross-validation.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you should be able to:\n",
    "\n",
    "* Build multiple types of classification models.\n",
    "* Evaluate the performance of each model using cross-validation.\n",
    "* Determine which model performs best for the MBTI dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q11CmP7yYDo6"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "In this train, we will throw some of the most common classifiers at the MBTI problem!  Let's not worry too much about what each algorithm is doing. Instead, we will focus on how to tell which model is likely to be better at solving this problem. To do so, we will fit the following classifiers to the MBTI dataset:\n",
    "\n",
    "* Logistic regression\n",
    "* K-nearest neighbours (KNN)\n",
    "* Support vector machines (SVM)\n",
    "* Decision trees\n",
    "* [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)\n",
    "\n",
    "Then, we will leave it up to you to play with a few additional models:\n",
    "\n",
    "* [Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "* [LDA / QDA](https://scikit-learn.org/stable/modules/lda_qda.html)\n",
    "* [Neural Network](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwPEmGUjYDo7"
   },
   "source": [
    "This approach highlights a key principle in machine learning.  The best models are built through **iteration**. We need to be able to implement a 'quick and dirty' model that should give us a good idea of how to proceed in finding a better solution to a particular problem. The quicker we can generate a couple of results, the quicker we can come up with new ideas of how to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_f2sVrEkYDo8"
   },
   "source": [
    "### Imports\n",
    "To start, lets import everything we will require. This will be some feature extraction methods for text, methods to split our data, and all the models we want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MJDmyj6bYDo9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NYMiKmMYDpA"
   },
   "source": [
    "### Data Pre-Processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8jT83XmYDpB"
   },
   "source": [
    "You should be familiar with these steps from previous trains on handling text data, specifically for the MBTI dataset.\n",
    "\n",
    "**Note:** Depending on the machine you're using and the quality of the internet connection, the following cell may take several minutes to execute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UqTRGxl5YDpC"
   },
   "outputs": [],
   "source": [
    "## Read the data\n",
    "print ('Reading the data into Pandas DF...')\n",
    "mbti = pd.read_csv('https://raw.githubusercontent.com/Explore-AI/Public-Data/master/Data/classification_sprint/mbti_train.csv')\n",
    "\n",
    "## Split the rows\n",
    "# Separate each post in the 'posts' column into individual rows and create a new DataFrame\n",
    "print ('Seperating data...')\n",
    "all_mbti = []\n",
    "for i, row in mbti.iterrows():\n",
    "    for post in row['posts'].split('|||'):\n",
    "        all_mbti.append([row['type'], post])\n",
    "all_mbti = pd.DataFrame(all_mbti, columns=['type', 'post'])\n",
    "\n",
    "## Remove urls\n",
    "# Replace URLs in the 'post' column with a generic label\n",
    "print ('Removing URLs...')\n",
    "pattern_url = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "subs_url = r'url-web'\n",
    "all_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n",
    "\n",
    "# Make lower case\n",
    "# Convert all text in the 'post' column to lowercase\n",
    "print ('Lowering case...')\n",
    "all_mbti['post'] = all_mbti['post'].str.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "# Define a function to remove punctuation and numbers from the 'post' column\n",
    "import string\n",
    "print ('Cleaning punctuation...')\n",
    "def remove_punctuation_numbers(post):\n",
    "    punc_numbers = string.punctuation + '0123456789'\n",
    "    return ''.join([l for l in post if l not in punc_numbers])\n",
    "# Apply the remove_punctuation_numbers function to the 'post' column\n",
    "all_mbti['post'] = all_mbti['post'].apply(remove_punctuation_numbers)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wHCjNLIyYDpE"
   },
   "source": [
    "For this example, we will only be looking at the Introvert / Extrovert dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "PGTGQn-bYDpF"
   },
   "outputs": [],
   "source": [
    "# Extract the first letter of the 'type' column to determine if the person is introverted (I) or not\n",
    "all_mbti['I'] = all_mbti['type'].apply(lambda x: x[0] == 'I').astype('int')\n",
    "# Assign the target variable to 'y'\n",
    "y = all_mbti['I']\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's fit the data to the Count Vectorizer, removing all English stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kc3IzFjDYDpP"
   },
   "outputs": [],
   "source": [
    "vect = CountVectorizer(stop_words='english', min_df= .01)\n",
    "X = vect.fit_transform(all_mbti['post'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7MRh4oelYDpZ"
   },
   "source": [
    "We have a lot of training data here!  To speed up the algorithms and illustrate the effects, we will only be using the first 5,000 rows of data.  Play around with the size of the data to get a feel of how each algorithm responds to more data from an accuracy and training time point of view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "G41671vLYDpZ"
   },
   "outputs": [],
   "source": [
    "n = 5000\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:n].toarray(), y[:n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzHon4BbYDph"
   },
   "source": [
    "## Let's build some classification models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gx85ydKUYDpi"
   },
   "source": [
    "In the next two cells, we will define the model names and call the model implementation classes. Note how some of the classifiers need input variables.  These are examples of **hyperparameters**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vJz6qixNYDpi"
   },
   "outputs": [],
   "source": [
    "# Define the names of the classifiers\n",
    "names = ['Logistic Regression', 'Nearest Neighbors', \n",
    "         'Linear SVM', 'RBF SVM',          \n",
    "         'Decision Tree', 'Random Forest',  'AdaBoost']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "03qpwMKHYDpl"
   },
   "outputs": [],
   "source": [
    "# Define the classifiers with their respective hyperparameters\n",
    "classifiers = [\n",
    "    LogisticRegression(), \n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),    \n",
    "    AdaBoostClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SVssWbc5YDpn"
   },
   "source": [
    "While this next cell is running, see if you can tell which algorithm takes the longest to train. Why do you think this is so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yDS6jA4iYDpo",
    "outputId": "02b6c333-66cd-4a59-d95f-6c55cd6b81ae"
   },
   "outputs": [],
   "source": [
    "# Empty lists to store results\n",
    "results = []  # Store evaluation metrics for each classifier\n",
    "models = {}  # Store trained models\n",
    "confusion = {}  # Store confusion matrices for each classifier\n",
    "class_report = {}  # Store classification reports for each classifier\n",
    "\n",
    "# Iterate over each classifier\n",
    "for name, clf in zip(names, classifiers):    \n",
    "    print ('Fitting {:s} model...'.format(name))\n",
    "    # Measure the time taken to fit the model\n",
    "    run_time = %timeit -q -o clf.fit(X_train, y_train)\n",
    "    \n",
    "    print ('... predicting')\n",
    "    # Predict on the training data\n",
    "    y_pred = clf.predict(X_train)   \n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    \n",
    "    print ('... scoring')\n",
    "    # Calculate evaluation metrics\n",
    "    accuracy  = metrics.accuracy_score(y_train, y_pred)\n",
    "    precision = metrics.precision_score(y_train, y_pred)\n",
    "    recall    = metrics.recall_score(y_train, y_pred)\n",
    "    \n",
    "    f1        = metrics.f1_score(y_train, y_pred)    \n",
    "    f1_test   = metrics.f1_score(y_test, y_pred_test)    \n",
    "    \n",
    "    # Save the results to dictionaries\n",
    "    models[name] = clf    \n",
    "    confusion[name] = metrics.confusion_matrix(y_train, y_pred)\n",
    "    class_report[name] = metrics.classification_report(y_train, y_pred)\n",
    "    \n",
    "    # Append results to the list\n",
    "    results.append([name, accuracy, precision, recall, f1, f1_test, run_time.best])\n",
    "\n",
    "    \n",
    "# Convert results to DataFrame for easy visualisation\n",
    "results = pd.DataFrame(results, columns=['Classifier', 'Accuracy', 'Precision', 'Recall', 'F1 Train', 'F1 Test', 'Train Time'])\n",
    "results.set_index('Classifier', inplace= True)\n",
    "\n",
    "print ('... All done!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "AvvD9qKnYDpr",
    "outputId": "46fcff2c-89da-441c-a10c-d31a919ea71c"
   },
   "outputs": [],
   "source": [
    "results.sort_values('F1 Train', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is way too many numbers to comprehend. Let's plot these values to see if we can make sense of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0ei8g9ApYDpu",
    "outputId": "8579c9e3-953e-4e85-9d36-83dbf53c67b5"
   },
   "outputs": [],
   "source": [
    "# Plot F1 Score on Test Data vs. Classifier\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "results.sort_values('F1 Train', ascending=False, inplace=True)\n",
    "results.plot(y=['F1 Test'], kind='bar', ax=ax[0], xlim=[0,1.1], ylim=[0.80,0.92])\n",
    "ax[0].set_title('F1 Score on Test Data vs. Classifier')\n",
    "results.plot(y='Train Time', kind='bar', ax=ax[1])\n",
    "ax[1].set_title('Training Time vs. Classifier')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand the performance of the models, let's view the confusion matrices and classification reports of their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BZTwdbEfYDpw",
    "outputId": "bf419628-daa0-45e9-ae9b-bacfdc8fc0b5"
   },
   "outputs": [],
   "source": [
    "# Display confusion matrices and classification reports\n",
    "for name, matrix in confusion.items():\n",
    "    print(f\"Confusion Matrix for {name}:\")\n",
    "    print(matrix)\n",
    "    print()\n",
    "    \n",
    "for name, report in class_report.items():\n",
    "    print(f\"Classification Report for {name}:\")\n",
    "    print(report)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you use the above metrics to analyse the performance of the models? Which model do you think outperforms the rest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eo_uOVZ4YDp6"
   },
   "source": [
    "## Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaXRy1z0YDp6"
   },
   "source": [
    "But how do we know if these models are robust?  \n",
    "\n",
    "Model validation is the process of checking if our model produces reliable results. In order to make an informed choice, we need a way to *validate* that our model and our hyperparameters are a good fit to the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rq6aJcEJYDp7"
   },
   "source": [
    "### K-Fold Cross Validation\n",
    "\n",
    "In the rest of this train, we consider a common approach to validation called `k-fold cross validation`.\n",
    "\n",
    "`K-fold cross-validation` is a technique used to evaluate the performance of machine learning models. It involves partitioning the dataset into k equal-sized folds, where each fold is used as a validation set while the remaining k-1 folds are used for training the model. This process is repeated k times, with each fold used exactly once as the validation data. \n",
    "\n",
    "The advantage of k-fold cross-validation is that it provides a **more reliable estimate of model performance** compared to a single train-test split. It helps to reduce the variance of the evaluation metrics, especially when the dataset is small or when the train-test split may not be representative of the entire dataset.\n",
    "\n",
    "In each iteration of k-fold cross-validation, the model is trained on `k-1 folds` and evaluated on the remaining fold. This process allows for a more comprehensive assessment of the model's performance across different subsets of the data.\n",
    "\n",
    "After performing k-fold cross-validation, the `performance metrics` (e.g., accuracy, precision, recall) obtained from each fold are averaged to obtain a single estimate of the model's performance. Additionally, the standard deviation of the metrics can provide insights into the variability of the model's performance across different folds.\n",
    "\n",
    "Overall, k-fold cross-validation is a valuable technique for assessing the generalisation ability of machine learning models and can help in selecting the best model and hyperparameters for a given dataset.\n",
    "\n",
    "Cross-validation can be easily implemented using `sklearn`. Let's do that for our Logistic Regression model and print out the performance of each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "zod5T5cDYDqA",
    "outputId": "87cbce18-a9de-4050-f4c3-496508a2c488"
   },
   "outputs": [],
   "source": [
    "# Retrieve the trained Logistic Regression model from the 'models' dictionary\n",
    "model = models['Logistic Regression']\n",
    "\n",
    "# Perform k-fold cross-validation on the Logistic Regression model\n",
    "# using the entire dataset (first n samples) and the target variable y\n",
    "# Print the cross-validation scores\n",
    "print(cross_val_score(model, X[:n].toarray(), y[:n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now do the same for each of our models and print out the mean and standard deviation of each model's cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yf5TlVRUYDqD",
    "outputId": "63a5e18b-7d1a-469b-dfc9-49b73f605819"
   },
   "outputs": [],
   "source": [
    "# Initialize an empty list to store cross-validation results\n",
    "cv = []\n",
    "\n",
    "# Iterate over each model in the 'models' dictionary\n",
    "for name, model in models.items():\n",
    "    print()  # Print an empty line for better readability\n",
    "    print(name)  # Print the name of the current model\n",
    "    # Perform k-fold cross-validation (with k=10) on the current model\n",
    "    scores = cross_val_score(model, X=X[:n].toarray(), y=y[:n], cv=10)\n",
    "    # Calculate the mean and standard deviation of the cross-validation scores\n",
    "    print(\"Accuracy: {:0.2f} (+/- {:0.4f})\".format(scores.mean(), scores.std()))\n",
    "    # Append the model name, mean cross-validation score, and standard deviation to the 'cv' list\n",
    "    cv.append([name, scores.mean(), scores.std()])\n",
    "\n",
    "# Convert the list of cross-validation results to a DataFrame\n",
    "cv = pd.DataFrame(cv, columns=['Model', 'CV_Mean', 'CV_Std_Dev'])\n",
    "\n",
    "# Set the index of the DataFrame to the model names\n",
    "cv.set_index('Model', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare these results using a bar chart. Which model do you think is best suited to our dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XPgMGPvrYDqG",
    "outputId": "7540b3c3-0612-468b-ba12-4a3404972692"
   },
   "outputs": [],
   "source": [
    "# Plot the mean cross-validation scores with error bars representing the standard deviation\n",
    "# Set the y-axis to represent the mean cross-validation scores\n",
    "# Set the error bars to represent the standard deviation of the cross-validation scores\n",
    "# Limit the y-axis to the range [0.65, 0.85] for better visualization\n",
    "cv.plot(y='CV_Mean', yerr='CV_Std_Dev', kind='bar', ylim=[0.65, 0.85])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can see that logistic regression, the SVM variants, and the Random Forest model all perform similarly and post good metric scores. On the other hand, our KNN classifier performed by far the worst. This can be expected seeing that we arbitrarily chose the value of `k` as 3 for that model.  \n",
    "\n",
    "This last point brings us back full circle to the introduction, where we spoke about **iteration**. Having performed our initial analysis, we can now ask several additional questions about our modelling problem:\n",
    "\n",
    " - Should we continue to investigate the performance of the KNN model under different settings of `k`? \n",
    " - Seeing that most of the scores are very similar, should we try to use additional metrics to better differentiate model performance? Perhaps we just need to add some more of our original data?  \n",
    " - Is performance the only aspect we care about for our given task? Should we focus on other model characteristics such as execution time, or memory consumption?   \n",
    " - Are the current scores good enough? If not, should we investigate more models or instead focus on improving the current ones through techniques such as hyperparameter tuning? \n",
    " \n",
    "As we can see, there are numerous questions to consider, and this list is by no means exhaustive. Developing proficiency as a data scientist involves discerning which questions are most relevant to the specific task at hand. This skill requires practice and the utilisation of your analytical abilities.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "That is a lot of classifiers! In this train we built seven different classification models and trained these on the MBTI dataset. To ensure that we got a robust measure of classifier performance, we then applied cross-validation to these models. Performing this analysis allowed us to reflect on our problem at hand; generating further questions to lead us along the iterative process of improving our model-based solution.  \n",
    "\n",
    "While we've explored several classification models, there are still additional models left for further experimentation. We encourage you to explore the performance of models such as `Naive Bayes`, `LDA / QDA`, and `Neural Networks` on the dataset. These models offer different approaches and complexities that may yield valuable insights or improved performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  \n",
    "\n",
    "<div align=\"center\" style=\" font-size: 80%; text-align: center; margin: 0 auto\">\n",
    "<img src=\"https://raw.githubusercontent.com/Explore-AI/Pictures/master/ExploreAI_logos/EAI_Blue_Dark.png\"  style=\"width:200px\";/>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_Build ALL the classifiers.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
